{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from load_data import *\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers import CamembertTokenizer\n",
    "\n",
    "raw_data = load_all_ipus(\"Dataset/transcr\")\n",
    "filepath = \"Dataset/audio/2_channels/\"\n",
    "\n",
    "wave2vec_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(wave2vec_name)\n",
    "\n",
    "bert_name = 'camembert-base'\n",
    "tokenizer = CamembertTokenizer.from_pretrained(bert_name)\n",
    "\n",
    "def get_audio(i, raw_data, filepath):\n",
    "    audio_file_path =  filepath + raw_data[\"dyad\"][i].replace(\"transcr\\\\\",\"\") + \".wav\"\n",
    "    audio_tensor, sampling_rate = torchaudio.load(audio_file_path)\n",
    "    audio_tensor = processor(audio_tensor, return_tensors=\"pt\", sampling_rate = sampling_rate).input_values.squeeze(0)\n",
    "    stop = int(sampling_rate*raw_data[\"stop\"][i])\n",
    "    start = stop-sampling_rate*1\n",
    "    if start < 0:\n",
    "        sample_tensor = audio_tensor[:,0:stop]\n",
    "        sample_tensor = torch.cat((torch.zeros((2,abs(start))),sample_tensor),dim=1)\n",
    "    else:\n",
    "        sample_tensor = audio_tensor[:,start:stop]\n",
    "    return sample_tensor\n",
    "\n",
    "def get_text(i, raw_data):\n",
    "    text = raw_data[\"text\"][i]\n",
    "    text_tokenized = tokenizer(text, return_tensors=\"pt\")['input_ids']\n",
    "    text_tokenized = torch.cat((torch.tensor([[1]*20]),text_tokenized),dim=1)\n",
    "    text_tokenized = text_tokenized[:,-20:]\n",
    "    return text_tokenized.squeeze(0)\n",
    "\n",
    "def get_label(i, raw_data):\n",
    "    return raw_data[\"turn_after\"].astype(\"float32\")[i]\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    \n",
    "    def __init__(self, raw_data, filepath):\n",
    "        self.raw_data = raw_data\n",
    "        self.filepath = filepath\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {\"audio\" : get_audio(i, self.raw_data, self.filepath),\n",
    "                \"text\" : get_text(i, self.raw_data),\n",
    "                \"label\" : get_label(i, self.raw_data)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "    \n",
    "def create_dataloader(generator):\n",
    "    \n",
    "    dataloader = DataLoader(generator,\n",
    "                            batch_size=64,\n",
    "                            shuffle=True,\n",
    "                            drop_last=True,)\n",
    "    \n",
    "    return dataloader"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16400, 12)\n",
      "is_main_speaker      0.739207\n",
      "turn_at_start        0.184756\n",
      "turn_after           0.184756\n",
      "turn_start_word     10.250372\n",
      "yield_at_end         0.188415\n",
      "request_at_start     0.198537\n",
      "dtype: float64\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style\n",
    "from data_processing import *\n",
    "from transformers import Wav2Vec2Model\n",
    "from transformers import CamembertModel\n",
    "from icecream import ic\n",
    "\n",
    "########################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "wave2vec_name = \"facebook/wav2vec2-base-960h\"\n",
    "wave2vec_model = Wav2Vec2Model.from_pretrained(wave2vec_name)\n",
    "wave2vec_model = wave2vec_model.to(device)\n",
    "\n",
    "bert_name = 'camembert-base'\n",
    "bert_model = CamembertModel.from_pretrained(bert_name)\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "for param in wave2vec_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "########################################################################\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model using covolutional neural net architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, device):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define model components\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "###################\n",
    "        \n",
    "    def parameters_number(self):\n",
    "        return(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Define the forward pass using the model components\n",
    "        \n",
    "        input_data['audio'] = input_data['audio'].to(self.device)\n",
    "        input_data['text'] = input_data['text'].to(self.device)\n",
    "        input_data['label'] = input_data['label'].to(self.device)\n",
    "\n",
    "        channel0 = input_data['audio'][:,0,:]\n",
    "        channel1 = input_data['audio'][:,1,:]\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_model(channel0)\n",
    "        wave2vec_output1 = wave2vec_model(channel1)\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_output0.last_hidden_state\n",
    "        wave2vec_output1 = wave2vec_output1.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.nn.functional.max_pool1d(wave2vec_output0, kernel_size=6)\n",
    "        wave2vec_output1 = torch.nn.functional.max_pool1d(wave2vec_output1, kernel_size=6)\n",
    "        \n",
    "        bert_output = bert_model(input_data['text'])\n",
    "        bert_output = bert_output.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.flatten(wave2vec_output0, start_dim=1)\n",
    "        wave2vec_output1 = torch.flatten(wave2vec_output1, start_dim=1)\n",
    "        bert_output = torch.flatten(bert_output, start_dim=1)\n",
    "\n",
    "        # Concatenate or combine the outputs as needed\n",
    "        combined_output = torch.cat((wave2vec_output0, wave2vec_output1, bert_output), dim=1)\n",
    "\n",
    "        # Apply linear layers\n",
    "        linear1_output = torch.relu(self.linear1(combined_output))\n",
    "        final_output = self.linear2(self.dropout(linear1_output))\n",
    "\n",
    "        return torch.softmax(final_output, dim=1)\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        self.eval()\n",
    "\n",
    "        # Define the loss function\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        true_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_negative_1 = torch.tensor(0).to(self.device)\n",
    "        \n",
    "        true_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_negative_0 = torch.tensor(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Validating\")):\n",
    "                \n",
    "                output = self.forward(batch)\n",
    "                labels = batch[\"label\"].long()\n",
    "\n",
    "                loss = loss_function(output, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                \n",
    "                true_positive_1 += torch.sum((predicted == labels) * (labels == 1))\n",
    "                false_positive_1 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 1))\n",
    "                false_negative_1 += torch.sum(((1-predicted) == labels) * (labels == 1))\n",
    "                \n",
    "                true_positive_0 += torch.sum((predicted == labels) * (labels == 0))\n",
    "                false_positive_0 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 0))\n",
    "                false_negative_0 += torch.sum(((1-predicted) == labels) * (labels == 0))\n",
    "                \n",
    "        precision_1 = true_positive_1 / max((true_positive_1 + false_positive_1), 1)\n",
    "        recall_1 = true_positive_1 / max((true_positive_1 + false_negative_1), 1) \n",
    "        \n",
    "        precision_0 = true_positive_0 / max((true_positive_0 + false_positive_0), 1)\n",
    "        recall_0 = true_positive_0 / max((true_positive_0 + false_negative_0), 1) \n",
    "        \n",
    "        f1_1 = 2 * (precision_1 * recall_1) / max((precision_1 + recall_1), 1)\n",
    "        f1_0 = 2 * (precision_0 * recall_0) / max((precision_0 + recall_0), 1)\n",
    "        \n",
    "        print(f\"Classe {Fore.RED}0{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_0 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_0 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_0 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Classe {Fore.RED}1{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_1 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_1 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_1 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Score : {(f1_0*0.18 + f1_1*(1-0.18))}\")\n",
    "                        \n",
    "    def train_one_epoch(self, dataloader):\n",
    "        \n",
    "        self.train(True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        # loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        try:\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "                \n",
    "                try:\n",
    "                \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    output = self.forward(batch)\n",
    "                    labels = batch[\"label\"].long()\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    \n",
    "                    true_positive_0 = (output * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "                    false_positive_0 = ((1 - output) * labels.unsqueeze(1))[:, 0].sum()\n",
    "                    false_negative_0 = ((1 - output) * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "\n",
    "                    true_positive_1 = (output * labels.unsqueeze(1))[:, 1].sum()\n",
    "                    false_positive_1 = (output * (1 - labels).unsqueeze(1))[:, 1].sum()\n",
    "                    false_negative_1 = ((1 - output) * labels.unsqueeze(1))[:, 1].sum()\n",
    "\n",
    "                    epsilon = torch.tensor([1e-7]).to(self.device) # 1e-7\n",
    "                    \n",
    "                    precision_0 = true_positive_0 / (true_positive_0 + false_positive_0 + epsilon)\n",
    "                    recall_0 = true_positive_0 / (true_positive_0 + false_negative_0 + epsilon)\n",
    "                    f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0 + epsilon)\n",
    "\n",
    "                    precision_1 = true_positive_1 / (true_positive_1 + false_positive_1 + epsilon)\n",
    "                    recall_1 = true_positive_1 / (true_positive_1 + false_negative_1 + epsilon)\n",
    "                    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1 + epsilon)\n",
    "                \n",
    "                    loss = -torch.log((f1_0*0.18 + f1_1*(1-0.18)) + epsilon)\n",
    "                    \n",
    "                    # ic(output, labels)\n",
    "                    # ic(true_positive_0, false_positive_0, false_negative_0, true_positive_1, false_positive_1, false_negative_1)\n",
    "                    # ic(precision_0, recall_0, f1_0, precision_1, recall_1, f1_1)\n",
    "                    # print(\"Successfully trained a batch\")\n",
    "                    \n",
    "                    ic(loss)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # labels = batch[\"label\"].long()\n",
    "                    # loss = loss_function(output, labels)\n",
    "                    # loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                except StopIteration as e:\n",
    "                    print(f\"An error occurred: {e}\")   \n",
    "                    # Catch StopIteration and continue to the next batch\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            # Handle other exceptions if needed\n",
    "            print(f\"An error occurred: {e}\")      \n",
    "\n",
    "    def train_loop(self, generator, nb_epoch):\n",
    "        \n",
    "        self.to(self.device)\n",
    "\n",
    "        data = generator.raw_data\n",
    "\n",
    "        train_data = data.sample(frac=0.9,random_state=200)\n",
    "        test_data = data.drop(train_data.index)\n",
    "\n",
    "        test_data.reset_index(drop=True, inplace=True)\n",
    "        train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_generator = DataGenerator(test_data, filepath)\n",
    "        test_loader = create_dataloader(test_generator)\n",
    "        \n",
    "        for epoch_number in range(nb_epoch):\n",
    "\n",
    "            train_subdata = train_data.sample(frac=0.9,random_state=200)\n",
    "            val_subdata = train_data.drop(train_subdata.index)\n",
    "            \n",
    "            train_subdata.reset_index(drop=True, inplace=True)\n",
    "            val_subdata.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            train_subgenerator = DataGenerator(train_subdata, filepath)\n",
    "            val_subgenerator = DataGenerator(val_subdata, filepath)\n",
    "\n",
    "            train_loader = create_dataloader(train_subgenerator)\n",
    "            val_loader = create_dataloader(val_subgenerator)\n",
    "\n",
    "            print(\"\")    \n",
    "            print(f'{Fore.GREEN}EPOCH {epoch_number + 1}:{Style.RESET_ALL}')\n",
    "            \n",
    "            # Train for one epoch\n",
    "            self.train_one_epoch(train_loader)\n",
    "\n",
    "            # Validate on the validation subset\n",
    "            print(f'{Fore.CYAN}Validation :{Style.RESET_ALL}')\n",
    "            self.evaluate(val_loader)\n",
    "            print(f'{Fore.YELLOW}Test :{Style.RESET_ALL}')\n",
    "            self.evaluate(test_loader)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "generator = DataGenerator(raw_data.iloc[:2048], filepath)\n",
    "model = Model(27904, 16, 2, device)\n",
    "\n",
    "# dataloader = create_dataloader(generator)\n",
    "# model.evaluate(dataloader)\n",
    "\n",
    "print(\"Nombre de paramètres du model:\", model.parameters_number())\n",
    "\n",
    "model.train_loop(generator, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16400, 12)\n",
      "is_main_speaker      0.739207\n",
      "turn_at_start        0.184756\n",
      "turn_after           0.184756\n",
      "turn_start_word     10.250372\n",
      "yield_at_end         0.188415\n",
      "request_at_start     0.198537\n",
      "dtype: float64\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nombre de paramètres du model: 446514\n",
      "\n",
      "\u001b[32mEPOCH 1:\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "ic| loss: tensor([1.0399], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   4%|▍         | 1/25 [00:29<11:49, 29.56s/it]ic| loss: tensor([1.1900], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   8%|▊         | 2/25 [00:51<09:41, 25.29s/it]ic| loss: tensor([1.1519], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  12%|█▏        | 3/25 [01:13<08:42, 23.76s/it]ic| loss: tensor([1.1701], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  16%|█▌        | 4/25 [01:35<08:02, 22.97s/it]ic| loss: tensor([0.8514], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  20%|██        | 5/25 [02:07<08:44, 26.24s/it]ic| loss: tensor([1.2737], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  24%|██▍       | 6/25 [02:31<08:03, 25.45s/it]ic| loss: tensor([0.9569], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  28%|██▊       | 7/25 [02:53<07:17, 24.33s/it]ic| loss: tensor([0.9815], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  32%|███▏      | 8/25 [03:17<06:52, 24.27s/it]ic| loss: tensor([1.5522], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  36%|███▌      | 9/25 [03:50<07:10, 26.93s/it]ic| loss: tensor([0.8645], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  40%|████      | 10/25 [04:23<07:10, 28.72s/it]ic| loss: tensor([1.0158], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  44%|████▍     | 11/25 [04:44<06:10, 26.50s/it]ic| loss: tensor([0.8913], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  48%|████▊     | 12/25 [05:06<05:25, 25.05s/it]ic| loss: tensor([0.9397], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  52%|█████▏    | 13/25 [05:36<05:19, 26.59s/it]ic| loss: tensor([1.3696], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  56%|█████▌    | 14/25 [06:06<05:03, 27.62s/it]ic| loss: tensor([0.8076], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  60%|██████    | 15/25 [06:36<04:44, 28.44s/it]ic| loss: tensor([1.0602], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  64%|██████▍   | 16/25 [06:57<03:55, 26.20s/it]ic| loss: tensor([0.8483], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  68%|██████▊   | 17/25 [07:23<03:28, 26.10s/it]ic| loss: tensor([1.0439], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  72%|███████▏  | 18/25 [07:56<03:17, 28.25s/it]ic| loss: tensor([0.8304], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  76%|███████▌  | 19/25 [08:31<03:01, 30.20s/it]ic| loss: tensor([0.9378], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  80%|████████  | 20/25 [09:03<02:32, 30.58s/it]ic| loss: tensor([0.8276], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  84%|████████▍ | 21/25 [09:35<02:03, 30.95s/it]ic| loss: tensor([1.1078], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  88%|████████▊ | 22/25 [10:05<01:32, 30.95s/it]ic| loss: tensor([0.7550], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  92%|█████████▏| 23/25 [10:35<01:01, 30.64s/it]ic| loss: tensor([0.8171], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  96%|█████████▌| 24/25 [11:05<00:30, 30.48s/it]ic| loss: tensor([1.0835], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training: 100%|██████████| 25/25 [11:36<00:00, 27.85s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[36mValidation :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 2/2 [01:00<00:00, 30.06s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m88.31%\u001b[0m, Recall: \u001b[32m65.38%\u001b[0m, F1 Score: \u001b[32m75.14%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m29.41%\u001b[0m, Recall: \u001b[32m62.50%\u001b[0m, F1 Score: \u001b[32m36.76%\u001b[0m\n",
      "Score : 0.43671923875808716\n",
      "\u001b[33mTest :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 3/3 [01:14<00:00, 24.90s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m84.26%\u001b[0m, Recall: \u001b[32m58.33%\u001b[0m, F1 Score: \u001b[32m68.94%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m22.62%\u001b[0m, Recall: \u001b[32m52.78%\u001b[0m, F1 Score: \u001b[32m23.88%\u001b[0m\n",
      "Score : 0.31987133622169495\n",
      "\n",
      "\u001b[32mEPOCH 2:\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]ic| loss: tensor([0.8456], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   4%|▍         | 1/25 [00:21<08:34, 21.44s/it]ic| loss: tensor([0.8885], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   8%|▊         | 2/25 [00:43<08:16, 21.59s/it]ic| loss: tensor([1.0998], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  12%|█▏        | 3/25 [01:04<07:50, 21.41s/it]ic| loss: tensor([0.9241], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  16%|█▌        | 4/25 [01:25<07:26, 21.27s/it]ic| loss: tensor([1.1541], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  20%|██        | 5/25 [01:46<07:04, 21.23s/it]ic| loss: tensor([0.5877], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  24%|██▍       | 6/25 [02:07<06:41, 21.13s/it]ic| loss: tensor([0.9522], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  28%|██▊       | 7/25 [02:31<06:35, 21.95s/it]ic| loss: tensor([0.7250], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  32%|███▏      | 8/25 [03:01<06:58, 24.60s/it]ic| loss: tensor([1.2862], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  36%|███▌      | 9/25 [03:31<07:03, 26.45s/it]ic| loss: tensor([0.9803], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  40%|████      | 10/25 [04:01<06:52, 27.48s/it]ic| loss: tensor([0.9349], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  44%|████▍     | 11/25 [04:31<06:35, 28.25s/it]ic| loss: tensor([1.0483], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  48%|████▊     | 12/25 [05:01<06:12, 28.62s/it]ic| loss: tensor([0.8402], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  52%|█████▏    | 13/25 [05:30<05:47, 28.98s/it]ic| loss: tensor([0.7831], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  56%|█████▌    | 14/25 [06:00<05:21, 29.22s/it]ic| loss: tensor([0.9462], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  60%|██████    | 15/25 [06:30<04:53, 29.38s/it]ic| loss: tensor([0.8147], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  64%|██████▍   | 16/25 [07:00<04:25, 29.51s/it]ic| loss: tensor([1.1347], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  68%|██████▊   | 17/25 [07:30<03:57, 29.72s/it]ic| loss: tensor([0.9906], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  72%|███████▏  | 18/25 [08:00<03:28, 29.72s/it]ic| loss: tensor([0.9582], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  76%|███████▌  | 19/25 [08:27<02:54, 29.08s/it]ic| loss: tensor([0.9043], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  80%|████████  | 20/25 [08:49<02:13, 26.80s/it]ic| loss: tensor([1.0509], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  84%|████████▍ | 21/25 [09:16<01:47, 26.89s/it]ic| loss: tensor([0.8447], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  88%|████████▊ | 22/25 [09:46<01:23, 27.73s/it]ic| loss: tensor([0.7580], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  92%|█████████▏| 23/25 [10:15<00:56, 28.31s/it]ic| loss: tensor([0.9353], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  96%|█████████▌| 24/25 [10:45<00:28, 28.77s/it]ic| loss: tensor([0.8258], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training: 100%|██████████| 25/25 [11:14<00:00, 27.00s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[36mValidation :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 2/2 [00:57<00:00, 28.86s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m88.89%\u001b[0m, Recall: \u001b[32m61.54%\u001b[0m, F1 Score: \u001b[32m72.73%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m28.57%\u001b[0m, Recall: \u001b[32m66.67%\u001b[0m, F1 Score: \u001b[32m38.10%\u001b[0m\n",
      "Score : 0.4432900547981262\n",
      "\u001b[33mTest :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 3/3 [01:04<00:00, 21.39s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m85.42%\u001b[0m, Recall: \u001b[32m52.23%\u001b[0m, F1 Score: \u001b[32m64.82%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m21.88%\u001b[0m, Recall: \u001b[32m60.00%\u001b[0m, F1 Score: \u001b[32m26.25%\u001b[0m\n",
      "Score : 0.33192986249923706\n",
      "\n",
      "\u001b[32mEPOCH 3:\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]ic| loss: tensor([0.9439], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   4%|▍         | 1/25 [00:21<08:34, 21.44s/it]ic| loss: tensor([1.2937], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   8%|▊         | 2/25 [00:42<08:13, 21.44s/it]ic| loss: tensor([0.8635], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  12%|█▏        | 3/25 [01:04<07:51, 21.44s/it]ic| loss: tensor([0.7720], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  16%|█▌        | 4/25 [01:25<07:29, 21.41s/it]ic| loss: tensor([0.8483], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  20%|██        | 5/25 [01:47<07:08, 21.40s/it]ic| loss: tensor([0.8074], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  24%|██▍       | 6/25 [02:08<06:47, 21.44s/it]ic| loss: tensor([0.9003], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  28%|██▊       | 7/25 [02:29<06:24, 21.38s/it]ic| loss: tensor([0.6518], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  32%|███▏      | 8/25 [02:50<06:02, 21.29s/it]ic| loss: tensor([0.8530], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  36%|███▌      | 9/25 [03:12<05:40, 21.28s/it]ic| loss: tensor([0.6394], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  40%|████      | 10/25 [03:33<05:19, 21.33s/it]ic| loss: tensor([0.8790], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  44%|████▍     | 11/25 [03:54<04:57, 21.26s/it]ic| loss: tensor([1.0827], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  48%|████▊     | 12/25 [04:16<04:36, 21.29s/it]ic| loss: tensor([0.6736], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  52%|█████▏    | 13/25 [04:37<04:16, 21.41s/it]ic| loss: tensor([0.6389], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  56%|█████▌    | 14/25 [05:05<04:17, 23.44s/it]ic| loss: tensor([0.7536], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  60%|██████    | 15/25 [05:28<03:50, 23.08s/it]ic| loss: tensor([0.8291], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  64%|██████▍   | 16/25 [05:50<03:24, 22.75s/it]ic| loss: tensor([0.7736], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  68%|██████▊   | 17/25 [06:12<03:01, 22.63s/it]ic| loss: tensor([0.8248], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  72%|███████▏  | 18/25 [06:40<02:50, 24.34s/it]ic| loss: tensor([0.8981], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  76%|███████▌  | 19/25 [07:15<02:44, 27.41s/it]ic| loss: tensor([0.8074], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  80%|████████  | 20/25 [07:47<02:24, 28.84s/it]ic| loss: tensor([0.8546], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  84%|████████▍ | 21/25 [08:17<01:57, 29.25s/it]ic| loss: tensor([0.7368], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  88%|████████▊ | 22/25 [08:47<01:27, 29.33s/it]ic| loss: tensor([0.9184], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  92%|█████████▏| 23/25 [09:17<00:58, 29.49s/it]ic| loss: tensor([0.7353], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  96%|█████████▌| 24/25 [09:46<00:29, 29.53s/it]ic| loss: tensor([0.9856], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training: 100%|██████████| 25/25 [10:17<00:00, 24.69s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[36mValidation :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 2/2 [00:59<00:00, 29.61s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m88.00%\u001b[0m, Recall: \u001b[32m64.08%\u001b[0m, F1 Score: \u001b[32m74.16%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m30.19%\u001b[0m, Recall: \u001b[32m64.00%\u001b[0m, F1 Score: \u001b[32m38.64%\u001b[0m\n",
      "Score : 0.450343519449234\n",
      "\u001b[33mTest :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 3/3 [01:28<00:00, 29.35s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m86.14%\u001b[0m, Recall: \u001b[32m56.13%\u001b[0m, F1 Score: \u001b[32m67.97%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m25.27%\u001b[0m, Recall: \u001b[32m62.16%\u001b[0m, F1 Score: \u001b[32m31.42%\u001b[0m\n",
      "Score : 0.38000932335853577\n",
      "\n",
      "\u001b[32mEPOCH 4:\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]ic| loss: tensor([0.7116], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   4%|▍         | 1/25 [00:29<11:49, 29.55s/it]ic| loss: tensor([0.7996], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   8%|▊         | 2/25 [00:54<10:23, 27.09s/it]ic| loss: tensor([0.7394], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  12%|█▏        | 3/25 [01:16<08:57, 24.44s/it]ic| loss: tensor([0.9625], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  16%|█▌        | 4/25 [01:37<08:05, 23.14s/it]ic| loss: tensor([1.1238], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  20%|██        | 5/25 [01:58<07:31, 22.58s/it]ic| loss: tensor([0.8755], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  24%|██▍       | 6/25 [02:24<07:26, 23.48s/it]ic| loss: tensor([0.8405], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  28%|██▊       | 7/25 [02:48<07:08, 23.83s/it]ic| loss: tensor([0.6571], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  32%|███▏      | 8/25 [03:18<07:16, 25.68s/it]ic| loss: tensor([0.8209], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  36%|███▌      | 9/25 [03:48<07:11, 26.95s/it]ic| loss: tensor([0.6314], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  40%|████      | 10/25 [04:17<06:55, 27.72s/it]ic| loss: tensor([0.8350], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  44%|████▍     | 11/25 [04:47<06:36, 28.30s/it]ic| loss: tensor([0.8372], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  48%|████▊     | 12/25 [05:16<06:12, 28.66s/it]ic| loss: tensor([0.6782], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  52%|█████▏    | 13/25 [05:48<05:55, 29.62s/it]ic| loss: tensor([1.0657], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  56%|█████▌    | 14/25 [06:18<05:27, 29.73s/it]ic| loss: tensor([0.7252], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  60%|██████    | 15/25 [06:49<05:02, 30.24s/it]ic| loss: tensor([0.8512], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  64%|██████▍   | 16/25 [07:21<04:36, 30.75s/it]ic| loss: tensor([0.6239], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  68%|██████▊   | 17/25 [07:51<04:03, 30.49s/it]ic| loss: tensor([0.7765], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  72%|███████▏  | 18/25 [08:24<03:39, 31.30s/it]ic| loss: tensor([1.0494], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  76%|███████▌  | 19/25 [08:55<03:07, 31.21s/it]ic| loss: tensor([0.5850], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  80%|████████  | 20/25 [09:18<02:22, 28.53s/it]ic| loss: tensor([0.7613], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  84%|████████▍ | 21/25 [09:39<01:45, 26.41s/it]ic| loss: tensor([0.6451], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  88%|████████▊ | 22/25 [10:01<01:14, 24.95s/it]ic| loss: tensor([0.8432], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  92%|█████████▏| 23/25 [10:22<00:47, 23.89s/it]ic| loss: tensor([0.6687], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  96%|█████████▌| 24/25 [10:43<00:23, 23.08s/it]ic| loss: tensor([0.6528], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training: 100%|██████████| 25/25 [11:04<00:00, 26.60s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[36mValidation :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 2/2 [00:45<00:00, 22.75s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m87.36%\u001b[0m, Recall: \u001b[32m72.38%\u001b[0m, F1 Score: \u001b[32m79.17%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m29.27%\u001b[0m, Recall: \u001b[32m52.17%\u001b[0m, F1 Score: \u001b[32m30.54%\u001b[0m\n",
      "Score : 0.39293479919433594\n",
      "\u001b[33mTest :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 3/3 [01:05<00:00, 21.68s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m85.04%\u001b[0m, Recall: \u001b[32m69.23%\u001b[0m, F1 Score: \u001b[32m76.33%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m26.15%\u001b[0m, Recall: \u001b[32m47.22%\u001b[0m, F1 Score: \u001b[32m24.70%\u001b[0m\n",
      "Score : 0.3399321734905243\n",
      "\n",
      "\u001b[32mEPOCH 5:\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]ic| loss: tensor([0.5867], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   4%|▍         | 1/25 [00:23<09:18, 23.27s/it]ic| loss: tensor([0.7087], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   8%|▊         | 2/25 [00:53<10:25, 27.18s/it]ic| loss: tensor([0.8408], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  12%|█▏        | 3/25 [01:23<10:26, 28.50s/it]ic| loss: tensor([0.7092], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  16%|█▌        | 4/25 [01:54<10:17, 29.39s/it]ic| loss: tensor([0.8783], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  20%|██        | 5/25 [02:24<09:55, 29.80s/it]ic| loss: tensor([0.6615], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  24%|██▍       | 6/25 [02:54<09:26, 29.82s/it]ic| loss: tensor([0.7652], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  28%|██▊       | 7/25 [03:24<08:57, 29.86s/it]ic| loss: tensor([0.7234], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  32%|███▏      | 8/25 [03:54<08:29, 29.99s/it]ic| loss: tensor([0.5219], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  36%|███▌      | 9/25 [04:25<08:02, 30.19s/it]ic| loss: tensor([0.6530], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  40%|████      | 10/25 [04:55<07:33, 30.25s/it]ic| loss: tensor([0.6080], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  44%|████▍     | 11/25 [05:26<07:04, 30.35s/it]ic| loss: tensor([0.8412], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  48%|████▊     | 12/25 [05:56<06:34, 30.36s/it]ic| loss: tensor([0.6859], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  52%|█████▏    | 13/25 [06:26<06:03, 30.30s/it]ic| loss: tensor([0.8331], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  56%|█████▌    | 14/25 [06:57<05:34, 30.40s/it]ic| loss: tensor([0.6437], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  60%|██████    | 15/25 [07:28<05:05, 30.52s/it]ic| loss: tensor([0.7418], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  64%|██████▍   | 16/25 [07:58<04:34, 30.50s/it]ic| loss: tensor([0.6979], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  68%|██████▊   | 17/25 [08:28<04:01, 30.23s/it]ic| loss: tensor([0.6435], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  72%|███████▏  | 18/25 [08:57<03:30, 30.01s/it]ic| loss: tensor([0.6744], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  76%|███████▌  | 19/25 [09:27<02:59, 29.86s/it]ic| loss: tensor([0.7367], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  80%|████████  | 20/25 [09:57<02:29, 29.92s/it]ic| loss: tensor([0.4826], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  84%|████████▍ | 21/25 [10:28<02:00, 30.22s/it]ic| loss: tensor([0.7154], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  88%|████████▊ | 22/25 [10:58<01:30, 30.24s/it]ic| loss: tensor([0.7048], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  92%|█████████▏| 23/25 [11:28<01:00, 30.14s/it]ic| loss: tensor([0.7281], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  96%|█████████▌| 24/25 [11:59<00:30, 30.39s/it]ic| loss: tensor([0.5146], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training: 100%|██████████| 25/25 [12:29<00:00, 29.99s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[36mValidation :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 2/2 [01:00<00:00, 30.19s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m84.69%\u001b[0m, Recall: \u001b[32m79.05%\u001b[0m, F1 Score: \u001b[32m81.77%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m26.67%\u001b[0m, Recall: \u001b[32m34.78%\u001b[0m, F1 Score: \u001b[32m18.55%\u001b[0m\n",
      "Score : 0.29930806159973145\n",
      "\u001b[33mTest :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 3/3 [01:20<00:00, 26.97s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m84.89%\u001b[0m, Recall: \u001b[32m75.64%\u001b[0m, F1 Score: \u001b[32m80.00%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m28.30%\u001b[0m, Recall: \u001b[32m41.67%\u001b[0m, F1 Score: \u001b[32m23.58%\u001b[0m\n",
      "Score : 0.33739620447158813\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from load_data import *\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers import CamembertTokenizer\n",
    "\n",
    "raw_data = load_all_ipus(\"Dataset/transcr\")\n",
    "filepath = \"Dataset/audio/2_channels/\"\n",
    "\n",
    "wave2vec_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(wave2vec_name)\n",
    "\n",
    "bert_name = 'camembert-base'\n",
    "tokenizer = CamembertTokenizer.from_pretrained(bert_name)\n",
    "\n",
    "def get_audio(i, raw_data, filepath):\n",
    "    audio_file_path =  filepath + raw_data[\"dyad\"][i].replace(\"transcr\\\\\",\"\") + \".wav\"\n",
    "    audio_tensor, sampling_rate = torchaudio.load(audio_file_path)\n",
    "    audio_tensor = processor(audio_tensor, return_tensors=\"pt\", sampling_rate = sampling_rate).input_values.squeeze(0)\n",
    "    stop = int(sampling_rate*raw_data[\"stop\"][i])\n",
    "    start = stop-sampling_rate*1\n",
    "    if start < 0:\n",
    "        sample_tensor = audio_tensor[:,0:stop]\n",
    "        sample_tensor = torch.cat((torch.zeros((2,abs(start))),sample_tensor),dim=1)\n",
    "    else:\n",
    "        sample_tensor = audio_tensor[:,start:stop]\n",
    "    return sample_tensor\n",
    "\n",
    "def get_text(i, raw_data):\n",
    "    text = raw_data[\"text\"][i]\n",
    "    text_tokenized = tokenizer(text, return_tensors=\"pt\")['input_ids']\n",
    "    text_tokenized = torch.cat((torch.tensor([[1]*20]),text_tokenized),dim=1)\n",
    "    text_tokenized = text_tokenized[:,-20:]\n",
    "    return text_tokenized.squeeze(0)\n",
    "\n",
    "def get_label(i, raw_data):\n",
    "    return raw_data[\"turn_after\"].astype(\"float32\")[i]\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    \n",
    "    def __init__(self, raw_data, filepath):\n",
    "        self.raw_data = raw_data\n",
    "        self.filepath = filepath\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {\"audio\" : get_audio(i, self.raw_data, self.filepath),\n",
    "                \"text\" : get_text(i, self.raw_data),\n",
    "                \"label\" : get_label(i, self.raw_data)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "    \n",
    "def create_dataloader(generator):\n",
    "    \n",
    "    dataloader = DataLoader(generator,\n",
    "                            batch_size=64,\n",
    "                            shuffle=True,\n",
    "                            drop_last=True,)\n",
    "    \n",
    "    return dataloader"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16400, 12)\n",
      "is_main_speaker      0.739207\n",
      "turn_at_start        0.184756\n",
      "turn_after           0.184756\n",
      "turn_start_word     10.250372\n",
      "yield_at_end         0.188415\n",
      "request_at_start     0.198537\n",
      "dtype: float64\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from load_data import *\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers import CamembertTokenizer\n",
    "\n",
    "raw_data = load_all_ipus(\"Dataset/transcr\")\n",
    "filepath = \"Dataset/audio/2_channels/\"\n",
    "\n",
    "wave2vec_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(wave2vec_name)\n",
    "\n",
    "bert_name = 'camembert-base'\n",
    "tokenizer = CamembertTokenizer.from_pretrained(bert_name)\n",
    "\n",
    "def get_audio(i, raw_data, filepath):\n",
    "    audio_file_path =  filepath + raw_data[\"dyad\"][i].replace(\"transcr\\\\\",\"\") + \".wav\"\n",
    "    audio_tensor, sampling_rate = torchaudio.load(audio_file_path)\n",
    "    audio_tensor = processor(audio_tensor, return_tensors=\"pt\", sampling_rate = sampling_rate).input_values.squeeze(0)\n",
    "    stop = int(sampling_rate*raw_data[\"stop\"][i])\n",
    "    start = stop-sampling_rate*1\n",
    "    if start < 0:\n",
    "        sample_tensor = audio_tensor[:,0:stop]\n",
    "        sample_tensor = torch.cat((torch.zeros((2,abs(start))),sample_tensor),dim=1)\n",
    "    else:\n",
    "        sample_tensor = audio_tensor[:,start:stop]\n",
    "    return sample_tensor\n",
    "\n",
    "def get_text(i, raw_data):\n",
    "    text = raw_data[\"text\"][i]\n",
    "    text_tokenized = tokenizer(text, return_tensors=\"pt\")['input_ids']\n",
    "    text_tokenized = torch.cat((torch.tensor([[1]*20]),text_tokenized),dim=1)\n",
    "    text_tokenized = text_tokenized[:,-20:]\n",
    "    return text_tokenized.squeeze(0)\n",
    "\n",
    "def get_label(i, raw_data):\n",
    "    return raw_data[\"turn_after\"].astype(\"float32\")[i]\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    \n",
    "    def __init__(self, raw_data, filepath):\n",
    "        self.raw_data = raw_data\n",
    "        self.filepath = filepath\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {\"audio\" : get_audio(i, self.raw_data, self.filepath),\n",
    "                \"text\" : get_text(i, self.raw_data),\n",
    "                \"label\" : get_label(i, self.raw_data)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "    \n",
    "def create_dataloader(generator):\n",
    "    \n",
    "    dataloader = DataLoader(generator,\n",
    "                            batch_size=64,\n",
    "                            shuffle=True,\n",
    "                            drop_last=True,)\n",
    "    \n",
    "    return dataloader"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16400, 12)\n",
      "is_main_speaker      0.739207\n",
      "turn_at_start        0.184756\n",
      "turn_after           0.184756\n",
      "turn_start_word     10.250372\n",
      "yield_at_end         0.188415\n",
      "request_at_start     0.198537\n",
      "dtype: float64\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "generator = DataGenerator(raw_data.iloc[2048:2048*3], filepath)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model.train_loop(generator, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u001b[32mEPOCH 1:\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/51 [00:00<?, ?it/s]ic| loss: tensor([0.9636], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   2%|▏         | 1/51 [00:30<25:03, 30.07s/it]ic| loss: tensor([1.2293], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   4%|▍         | 2/51 [01:05<27:12, 33.31s/it]ic| loss: tensor([0.8569], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   6%|▌         | 3/51 [01:28<23:00, 28.76s/it]ic| loss: tensor([1.0114], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   8%|▊         | 4/51 [01:55<21:53, 27.94s/it]ic| loss: tensor([1.2257], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  10%|▉         | 5/51 [02:36<23:58, 31.28s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "An error occurred: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "\u001b[36mValidation :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 5/5 [02:41<00:00, 32.21s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m84.49%\u001b[0m, Recall: \u001b[32m80.23%\u001b[0m, F1 Score: \u001b[32m82.31%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m32.00%\u001b[0m, Recall: \u001b[32m38.71%\u001b[0m, F1 Score: \u001b[32m24.77%\u001b[0m\n",
      "Score : 0.3512994647026062\n",
      "\u001b[33mTest :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 6/6 [03:00<00:00, 30.06s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m85.37%\u001b[0m, Recall: \u001b[32m78.44%\u001b[0m, F1 Score: \u001b[32m81.76%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m23.33%\u001b[0m, Recall: \u001b[32m32.81%\u001b[0m, F1 Score: \u001b[32m15.31%\u001b[0m\n",
      "Score : 0.2727286219596863\n",
      "\n",
      "\u001b[32mEPOCH 2:\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/51 [00:00<?, ?it/s]ic| loss: tensor([0.7594], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   2%|▏         | 1/51 [00:23<19:59, 23.99s/it]ic| loss: tensor([0.9254], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   4%|▍         | 2/51 [00:50<20:40, 25.32s/it]ic| loss: tensor([0.8800], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   6%|▌         | 3/51 [01:20<22:06, 27.63s/it]ic| loss: tensor([1.0161], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   8%|▊         | 4/51 [01:44<20:36, 26.31s/it]ic| loss: tensor([0.9283], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  10%|▉         | 5/51 [02:09<19:38, 25.63s/it]ic| loss: tensor([0.9517], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  12%|█▏        | 6/51 [02:33<18:48, 25.09s/it]ic| loss: tensor([0.9378], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  14%|█▎        | 7/51 [03:01<19:07, 26.09s/it]ic| loss: tensor([0.7265], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  16%|█▌        | 8/51 [03:28<18:56, 26.43s/it]ic| loss: tensor([0.9942], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  18%|█▊        | 9/51 [04:01<18:48, 26.86s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "An error occurred: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "\u001b[36mValidation :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 5/5 [02:05<00:00, 25.15s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m84.33%\u001b[0m, Recall: \u001b[32m87.60%\u001b[0m, F1 Score: \u001b[32m85.93%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m38.46%\u001b[0m, Recall: \u001b[32m32.26%\u001b[0m, F1 Score: \u001b[32m24.81%\u001b[0m\n",
      "Score : 0.35815075039863586\n",
      "\u001b[33mTest :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 6/6 [02:57<00:00, 29.66s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m85.62%\u001b[0m, Recall: \u001b[32m85.09%\u001b[0m, F1 Score: \u001b[32m85.36%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m25.00%\u001b[0m, Recall: \u001b[32m25.81%\u001b[0m, F1 Score: \u001b[32m12.90%\u001b[0m\n",
      "Score : 0.259451299905777\n",
      "\n",
      "\u001b[32mEPOCH 3:\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/51 [00:00<?, ?it/s]ic| loss: tensor([0.8643], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   2%|▏         | 1/51 [00:39<33:02, 39.65s/it]ic| loss: tensor([0.9456], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   4%|▍         | 2/51 [01:08<27:10, 33.27s/it]ic| loss: tensor([0.8322], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   6%|▌         | 3/51 [01:39<25:45, 32.20s/it]ic| loss: tensor([0.7710], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   8%|▊         | 4/51 [02:12<25:25, 32.47s/it]ic| loss: tensor([1.0474], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  10%|▉         | 5/51 [02:43<24:25, 31.85s/it]ic| loss: tensor([1.3760], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  12%|█▏        | 6/51 [03:12<23:19, 31.11s/it]ic| loss: tensor([1.0645], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  14%|█▎        | 7/51 [03:42<22:32, 30.75s/it]ic| loss: tensor([1.0095], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  16%|█▌        | 8/51 [04:11<21:42, 30.29s/it]ic| loss: tensor([0.7748], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  18%|█▊        | 9/51 [04:44<21:40, 30.96s/it]ic| loss: tensor([1.1054], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  20%|█▉        | 10/51 [05:17<21:36, 31.61s/it]ic| loss: tensor([1.1012], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  22%|██▏       | 11/51 [05:51<21:29, 32.24s/it]ic| loss: tensor([0.8144], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  24%|██▎       | 12/51 [06:25<21:23, 32.90s/it]ic| loss: tensor([0.9651], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  25%|██▌       | 13/51 [06:58<20:51, 32.94s/it]ic| loss: tensor([1.0686], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  27%|██▋       | 14/51 [07:30<20:10, 32.72s/it]ic| loss: tensor([0.8116], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  29%|██▉       | 15/51 [08:04<19:45, 32.92s/it]ic| loss: tensor([0.8543], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  31%|███▏      | 16/51 [08:37<19:19, 33.13s/it]ic| loss: tensor([0.8720], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  33%|███▎      | 17/51 [09:04<17:39, 31.15s/it]ic| loss: tensor([0.6953], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  35%|███▌      | 18/51 [09:32<16:38, 30.25s/it]ic| loss: tensor([0.8770], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  37%|███▋      | 19/51 [10:05<16:32, 31.03s/it]ic| loss: tensor([1.2067], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  39%|███▉      | 20/51 [10:44<17:15, 33.40s/it]ic| loss: tensor([0.8793], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  41%|████      | 21/51 [11:28<18:18, 36.62s/it]ic| loss: tensor([0.5286], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  43%|████▎     | 22/51 [12:12<18:42, 38.72s/it]ic| loss: tensor([0.7150], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  45%|████▌     | 23/51 [12:47<17:37, 37.75s/it]ic| loss: tensor([0.7442], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  47%|████▋     | 24/51 [13:52<15:36, 34.67s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "An error occurred: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "\u001b[36mValidation :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 5/5 [03:21<00:00, 40.25s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m87.36%\u001b[0m, Recall: \u001b[32m58.69%\u001b[0m, F1 Score: \u001b[32m70.21%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m26.71%\u001b[0m, Recall: \u001b[32m63.93%\u001b[0m, F1 Score: \u001b[32m34.16%\u001b[0m\n",
      "Score : 0.4064594507217407\n",
      "\u001b[33mTest :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 6/6 [03:11<00:00, 31.90s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m88.18%\u001b[0m, Recall: \u001b[32m55.76%\u001b[0m, F1 Score: \u001b[32m68.32%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m21.55%\u001b[0m, Recall: \u001b[32m61.90%\u001b[0m, F1 Score: \u001b[32m26.68%\u001b[0m\n",
      "Score : 0.3417300581932068\n",
      "\n",
      "\u001b[32mEPOCH 4:\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/51 [00:00<?, ?it/s]ic| loss: tensor([0.8782], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   2%|▏         | 1/51 [00:35<29:25, 35.31s/it]ic| loss: tensor([0.9013], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   4%|▍         | 2/51 [01:10<28:34, 34.99s/it]ic| loss: tensor([0.7270], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   6%|▌         | 3/51 [01:44<27:50, 34.81s/it]ic| loss: tensor([0.6767], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   8%|▊         | 4/51 [02:18<27:03, 34.54s/it]ic| loss: tensor([0.9790], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  10%|▉         | 5/51 [02:53<26:25, 34.47s/it]ic| loss: tensor([0.6718], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  12%|█▏        | 6/51 [03:27<25:47, 34.39s/it]ic| loss: tensor([0.8697], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  14%|█▎        | 7/51 [04:01<25:15, 34.44s/it]ic| loss: tensor([0.9754], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  16%|█▌        | 8/51 [04:36<24:46, 34.57s/it]ic| loss: tensor([0.9184], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  18%|█▊        | 9/51 [05:11<24:11, 34.57s/it]ic| loss: tensor([0.7147], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  20%|█▉        | 10/51 [05:45<23:33, 34.48s/it]ic| loss: tensor([0.8278], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  22%|██▏       | 11/51 [06:21<23:18, 34.95s/it]ic| loss: tensor([0.7900], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  24%|██▎       | 12/51 [06:56<22:36, 34.78s/it]ic| loss: tensor([1.0810], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  25%|██▌       | 13/51 [07:30<22:01, 34.77s/it]ic| loss: tensor([0.9167], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  27%|██▋       | 14/51 [08:04<21:19, 34.59s/it]ic| loss: tensor([1.0362], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  29%|██▉       | 15/51 [08:39<20:41, 34.49s/it]ic| loss: tensor([1.0159], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  31%|███▏      | 16/51 [09:14<20:15, 34.74s/it]ic| loss: tensor([0.8672], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  33%|███▎      | 17/51 [09:49<19:46, 34.89s/it]ic| loss: tensor([0.9224], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  35%|███▌      | 18/51 [10:25<19:22, 35.22s/it]ic| loss: tensor([0.8427], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  37%|███▋      | 19/51 [11:02<18:58, 35.57s/it]ic| loss: tensor([1.1130], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  39%|███▉      | 20/51 [11:35<18:03, 34.95s/it]ic| loss: tensor([0.9400], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  41%|████      | 21/51 [12:10<17:28, 34.96s/it]ic| loss: tensor([0.8365], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  43%|████▎     | 22/51 [12:45<16:54, 35.00s/it]ic| loss: tensor([0.8596], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  45%|████▌     | 23/51 [13:19<16:12, 34.75s/it]ic| loss: tensor([0.6950], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  47%|████▋     | 24/51 [13:55<15:47, 35.10s/it]ic| loss: tensor([0.6943], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  49%|████▉     | 25/51 [14:29<15:02, 34.70s/it]ic| loss: tensor([0.8677], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  51%|█████     | 26/51 [15:01<14:03, 33.74s/it]ic| loss: tensor([1.1755], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  53%|█████▎    | 27/51 [15:26<12:26, 31.11s/it]ic| loss: tensor([0.7133], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  55%|█████▍    | 28/51 [15:50<11:06, 28.98s/it]ic| loss: tensor([0.9425], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  57%|█████▋    | 29/51 [16:12<09:53, 26.99s/it]ic| loss: tensor([0.5497], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  59%|█████▉    | 30/51 [16:36<09:08, 26.13s/it]ic| loss: tensor([1.1336], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  61%|██████    | 31/51 [17:15<11:08, 33.42s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "An error occurred: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "\u001b[36mValidation :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 5/5 [02:00<00:00, 24.07s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m84.72%\u001b[0m, Recall: \u001b[32m74.62%\u001b[0m, F1 Score: \u001b[32m79.35%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m27.47%\u001b[0m, Recall: \u001b[32m41.67%\u001b[0m, F1 Score: \u001b[32m22.89%\u001b[0m\n",
      "Score : 0.33055102825164795\n",
      "\u001b[33mTest :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating:   0%|          | 0/6 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mtrain_loop(generator, \u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[2], line 233\u001b[0m, in \u001b[0;36mModel.train_loop\u001b[1;34m(self, generator, nb_epoch)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(val_loader)\n\u001b[0;32m    232\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mFore\u001b[39m.\u001b[39mYELLOW\u001b[39m}\u001b[39;00m\u001b[39mTest :\u001b[39m\u001b[39m{\u001b[39;00mStyle\u001b[39m.\u001b[39mRESET_ALL\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 233\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(test_loader)\n",
      "Cell \u001b[1;32mIn[2], line 103\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m    100\u001b[0m false_negative_0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    102\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# Disable gradient computation during validation\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mfor\u001b[39;00m _, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(dataloader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidating\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m    105\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(batch)\n\u001b[0;32m    106\u001b[0m         labels \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mlong()\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1174'>1175</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1176'>1177</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1177'>1178</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1178'>1179</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1179'>1180</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1180'>1181</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=626'>627</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=627'>628</a>\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=628'>629</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=629'>630</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=630'>631</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=631'>632</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=632'>633</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=633'>634</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=671'>672</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=672'>673</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=673'>674</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=674'>675</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=675'>676</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=52'>53</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=52'>53</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\data_processing.py\u001b[0m in \u001b[0;36mline 47\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[1;34m(self, i)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=45'>46</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, i):\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m : get_audio(i, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath),\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=47'>48</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m : get_text(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_data),\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=48'>49</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m : get_label(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_data)}\n",
      "\u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\data_processing.py\u001b[0m in \u001b[0;36mline 20\u001b[0m, in \u001b[0;36mget_audio\u001b[1;34m(i, raw_data, filepath)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=17'>18</a>\u001b[0m audio_file_path \u001b[39m=\u001b[39m  filepath \u001b[39m+\u001b[39m raw_data[\u001b[39m\"\u001b[39m\u001b[39mdyad\u001b[39m\u001b[39m\"\u001b[39m][i]\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mtranscr\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.wav\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=18'>19</a>\u001b[0m audio_tensor, sampling_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mload(audio_file_path)\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=19'>20</a>\u001b[0m audio_tensor \u001b[39m=\u001b[39m processor(audio_tensor, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, sampling_rate \u001b[39m=\u001b[39;49m sampling_rate)\u001b[39m.\u001b[39minput_values\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=20'>21</a>\u001b[0m stop \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(sampling_rate\u001b[39m*\u001b[39mraw_data[\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m][i])\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=21'>22</a>\u001b[0m start \u001b[39m=\u001b[39m stop\u001b[39m-\u001b[39msampling_rate\u001b[39m*\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:94\u001b[0m, in \u001b[0;36mWav2Vec2Processor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=90'>91</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou need to specify either an `audio` or `text` input to process.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=92'>93</a>\u001b[0m \u001b[39mif\u001b[39;00m audio \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=93'>94</a>\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_extractor(audio, \u001b[39m*\u001b[39;49margs, sampling_rate\u001b[39m=\u001b[39;49msampling_rate, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=94'>95</a>\u001b[0m \u001b[39mif\u001b[39;00m text \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=95'>96</a>\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(text, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py:233\u001b[0m, in \u001b[0;36mWav2Vec2FeatureExtractor.__call__\u001b[1;34m(self, raw_speech, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors, sampling_rate, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=226'>227</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_normalize:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=227'>228</a>\u001b[0m     attention_mask \u001b[39m=\u001b[39m (\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=228'>229</a>\u001b[0m         attention_mask\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=229'>230</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_strategies(padding, max_length\u001b[39m=\u001b[39mmax_length) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=230'>231</a>\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=231'>232</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=232'>233</a>\u001b[0m     padded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_values\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzero_mean_unit_var_norm(\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=233'>234</a>\u001b[0m         padded_inputs[\u001b[39m\"\u001b[39;49m\u001b[39minput_values\u001b[39;49m\u001b[39m\"\u001b[39;49m], attention_mask\u001b[39m=\u001b[39;49mattention_mask, padding_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_value\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=234'>235</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=236'>237</a>\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=237'>238</a>\u001b[0m     padded_inputs \u001b[39m=\u001b[39m padded_inputs\u001b[39m.\u001b[39mconvert_to_tensors(return_tensors)\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py:98\u001b[0m, in \u001b[0;36mWav2Vec2FeatureExtractor.zero_mean_unit_var_norm\u001b[1;34m(input_values, attention_mask, padding_value)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=95'>96</a>\u001b[0m         normed_input_values\u001b[39m.\u001b[39mappend(normed_slice)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=96'>97</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=97'>98</a>\u001b[0m     normed_input_values \u001b[39m=\u001b[39m [(x \u001b[39m-\u001b[39;49m x\u001b[39m.\u001b[39;49mmean()) \u001b[39m/\u001b[39;49m np\u001b[39m.\u001b[39;49msqrt(x\u001b[39m.\u001b[39;49mvar() \u001b[39m+\u001b[39;49m \u001b[39m1e-7\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m input_values]\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=99'>100</a>\u001b[0m \u001b[39mreturn\u001b[39;00m normed_input_values\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py:98\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=95'>96</a>\u001b[0m         normed_input_values\u001b[39m.\u001b[39mappend(normed_slice)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=96'>97</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=97'>98</a>\u001b[0m     normed_input_values \u001b[39m=\u001b[39m [(x \u001b[39m-\u001b[39m x\u001b[39m.\u001b[39mmean()) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msqrt(x\u001b[39m.\u001b[39;49mvar() \u001b[39m+\u001b[39m \u001b[39m1e-7\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m input_values]\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=99'>100</a>\u001b[0m \u001b[39mreturn\u001b[39;00m normed_input_values\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:233\u001b[0m, in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=227'>228</a>\u001b[0m     arrmean \u001b[39m=\u001b[39m arrmean \u001b[39m/\u001b[39m rcount\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=229'>230</a>\u001b[0m \u001b[39m# Compute sum of squared deviations from mean\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=230'>231</a>\u001b[0m \u001b[39m# Note that x may not be inexact and that we need it to be an array,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=231'>232</a>\u001b[0m \u001b[39m# not a scalar.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=232'>233</a>\u001b[0m x \u001b[39m=\u001b[39m asanyarray(arr \u001b[39m-\u001b[39m arrmean)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=234'>235</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(arr\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, (nt\u001b[39m.\u001b[39mfloating, nt\u001b[39m.\u001b[39minteger)):\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=235'>236</a>\u001b[0m     x \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39mmultiply(x, x, out\u001b[39m=\u001b[39mx)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "torch.save(model, \"checkpoint1.pth\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style\n",
    "from data_processing import *\n",
    "from transformers import Wav2Vec2Model\n",
    "from transformers import CamembertModel\n",
    "from icecream import ic\n",
    "\n",
    "########################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "wave2vec_name = \"facebook/wav2vec2-base-960h\"\n",
    "wave2vec_model = Wav2Vec2Model.from_pretrained(wave2vec_name)\n",
    "wave2vec_model = wave2vec_model.to(device)\n",
    "\n",
    "bert_name = 'camembert-base'\n",
    "bert_model = CamembertModel.from_pretrained(bert_name)\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "for param in wave2vec_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "########################################################################\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model using covolutional neural net architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, device):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define model components\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "###################\n",
    "        \n",
    "    def parameters_number(self):\n",
    "        return(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Define the forward pass using the model components\n",
    "        \n",
    "        input_data['audio'] = input_data['audio'].to(self.device)\n",
    "        input_data['text'] = input_data['text'].to(self.device)\n",
    "        input_data['label'] = input_data['label'].to(self.device)\n",
    "\n",
    "        channel0 = input_data['audio'][:,0,:]\n",
    "        channel1 = input_data['audio'][:,1,:]\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_model(channel0)\n",
    "        wave2vec_output1 = wave2vec_model(channel1)\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_output0.last_hidden_state\n",
    "        wave2vec_output1 = wave2vec_output1.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.nn.functional.max_pool1d(wave2vec_output0, kernel_size=6)\n",
    "        wave2vec_output1 = torch.nn.functional.max_pool1d(wave2vec_output1, kernel_size=6)\n",
    "        \n",
    "        bert_output = bert_model(input_data['text'])\n",
    "        bert_output = bert_output.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.flatten(wave2vec_output0, start_dim=1)\n",
    "        wave2vec_output1 = torch.flatten(wave2vec_output1, start_dim=1)\n",
    "        bert_output = torch.flatten(bert_output, start_dim=1)\n",
    "\n",
    "        # Concatenate or combine the outputs as needed\n",
    "        combined_output = torch.cat((wave2vec_output0, wave2vec_output1, bert_output), dim=1)\n",
    "\n",
    "        # Apply linear layers\n",
    "        linear1_output = torch.relu(self.linear1(combined_output))\n",
    "        final_output = self.linear2(self.dropout(linear1_output))\n",
    "\n",
    "        return torch.softmax(final_output, dim=1)\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        self.eval()\n",
    "\n",
    "        # Define the loss function\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        true_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_negative_1 = torch.tensor(0).to(self.device)\n",
    "        \n",
    "        true_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_negative_0 = torch.tensor(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Validating\")):\n",
    "                \n",
    "                output = self.forward(batch)\n",
    "                labels = batch[\"label\"].long()\n",
    "\n",
    "                loss = loss_function(output, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                \n",
    "                true_positive_1 += torch.sum((predicted == labels) * (labels == 1))\n",
    "                false_positive_1 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 1))\n",
    "                false_negative_1 += torch.sum(((1-predicted) == labels) * (labels == 1))\n",
    "                \n",
    "                true_positive_0 += torch.sum((predicted == labels) * (labels == 0))\n",
    "                false_positive_0 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 0))\n",
    "                false_negative_0 += torch.sum(((1-predicted) == labels) * (labels == 0))\n",
    "                \n",
    "        precision_1 = true_positive_1 / max((true_positive_1 + false_positive_1), 1)\n",
    "        recall_1 = true_positive_1 / max((true_positive_1 + false_negative_1), 1) \n",
    "        \n",
    "        precision_0 = true_positive_0 / max((true_positive_0 + false_positive_0), 1)\n",
    "        recall_0 = true_positive_0 / max((true_positive_0 + false_negative_0), 1) \n",
    "        \n",
    "        f1_1 = 2 * (precision_1 * recall_1) / max((precision_1 + recall_1), 1)\n",
    "        f1_0 = 2 * (precision_0 * recall_0) / max((precision_0 + recall_0), 1)\n",
    "        \n",
    "        print(f\"Classe {Fore.RED}0{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_0 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_0 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_0 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Classe {Fore.RED}1{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_1 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_1 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_1 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Score : {(f1_0*0.18 + f1_1*(1-0.18))}\")\n",
    "                        \n",
    "    def train_one_epoch(self, dataloader):\n",
    "        \n",
    "        self.train(True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        # loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        try:\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "                \n",
    "                try:\n",
    "                \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    output = self.forward(batch)\n",
    "                    labels = batch[\"label\"].long()\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    \n",
    "                    true_positive_0 = (output * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "                    false_positive_0 = ((1 - output) * labels.unsqueeze(1))[:, 0].sum()\n",
    "                    false_negative_0 = ((1 - output) * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "\n",
    "                    true_positive_1 = (output * labels.unsqueeze(1))[:, 1].sum()\n",
    "                    false_positive_1 = (output * (1 - labels).unsqueeze(1))[:, 1].sum()\n",
    "                    false_negative_1 = ((1 - output) * labels.unsqueeze(1))[:, 1].sum()\n",
    "\n",
    "                    epsilon = torch.tensor([1e-7]).to(self.device) # 1e-7\n",
    "                    \n",
    "                    precision_0 = true_positive_0 / (true_positive_0 + false_positive_0 + epsilon)\n",
    "                    recall_0 = true_positive_0 / (true_positive_0 + false_negative_0 + epsilon)\n",
    "                    f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0 + epsilon)\n",
    "\n",
    "                    precision_1 = true_positive_1 / (true_positive_1 + false_positive_1 + epsilon)\n",
    "                    recall_1 = true_positive_1 / (true_positive_1 + false_negative_1 + epsilon)\n",
    "                    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1 + epsilon)\n",
    "                \n",
    "                    loss = -torch.log((f1_0*0.18 + f1_1*(1-0.18)) + epsilon)\n",
    "                    \n",
    "                    # ic(output, labels)\n",
    "                    # ic(true_positive_0, false_positive_0, false_negative_0, true_positive_1, false_positive_1, false_negative_1)\n",
    "                    # ic(precision_0, recall_0, f1_0, precision_1, recall_1, f1_1)\n",
    "                    # print(\"Successfully trained a batch\")\n",
    "                    \n",
    "                    ic(loss)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # labels = batch[\"label\"].long()\n",
    "                    # loss = loss_function(output, labels)\n",
    "                    # loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                except StopIteration as e:\n",
    "                    print(f\"An error occurred: {e}\")   \n",
    "                    # Catch StopIteration and continue to the next batch\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            # Handle other exceptions if needed\n",
    "            print(f\"An error occurred: {e}\")      \n",
    "\n",
    "    def train_loop(self, generator, nb_epoch):\n",
    "        \n",
    "        self.to(self.device)\n",
    "\n",
    "        data = generator.raw_data\n",
    "\n",
    "        train_data = data.sample(frac=0.9,random_state=200)\n",
    "        test_data = data.drop(train_data.index)\n",
    "\n",
    "        test_data.reset_index(drop=True, inplace=True)\n",
    "        train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_generator = DataGenerator(test_data, filepath)\n",
    "        test_loader = create_dataloader(test_generator)\n",
    "        \n",
    "        for epoch_number in range(nb_epoch):\n",
    "\n",
    "            train_subdata = train_data.sample(frac=0.9,random_state=200)\n",
    "            val_subdata = train_data.drop(train_subdata.index)\n",
    "            \n",
    "            train_subdata.reset_index(drop=True, inplace=True)\n",
    "            val_subdata.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            train_subgenerator = DataGenerator(train_subdata, filepath)\n",
    "            val_subgenerator = DataGenerator(val_subdata, filepath)\n",
    "\n",
    "            train_loader = create_dataloader(train_subgenerator)\n",
    "            val_loader = create_dataloader(val_subgenerator)\n",
    "\n",
    "            print(\"\")    \n",
    "            print(f'{Fore.GREEN}EPOCH {epoch_number + 1}:{Style.RESET_ALL}')\n",
    "            \n",
    "            # Train for one epoch\n",
    "            self.train_one_epoch(train_loader)\n",
    "\n",
    "            # Validate on the validation subset\n",
    "            print(f'{Fore.CYAN}Validation :{Style.RESET_ALL}')\n",
    "            self.evaluate(val_loader)\n",
    "            print(f'{Fore.YELLOW}Test :{Style.RESET_ALL}')\n",
    "            self.evaluate(test_loader)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "generator = DataGenerator(raw_data.iloc[2048:2048*3], filepath)\n",
    "model = Model(27904, 16, 2, device)\n",
    "\n",
    "# dataloader = create_dataloader(generator)\n",
    "# model.evaluate(dataloader)\n",
    "\n",
    "print(\"Nombre de paramètres du model:\", model.parameters_number())\n",
    "\n",
    "model.train_loop(generator, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nombre de paramètres du model: 446514\n",
      "\n",
      "\u001b[32mEPOCH 1:\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/51 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\model.py\u001b[0m in \u001b[0;36mline 245\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=239'>240</a>\u001b[0m \u001b[39m# dataloader = create_dataloader(generator)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=240'>241</a>\u001b[0m \u001b[39m# model.evaluate(dataloader)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=242'>243</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNombre de paramètres du model:\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m.\u001b[39mparameters_number())\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=244'>245</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain_loop(generator, \u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\model.py\u001b[0m in \u001b[0;36mline 227\u001b[0m, in \u001b[0;36mModel.train_loop\u001b[1;34m(self, generator, nb_epoch)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=223'>224</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mFore\u001b[39m.\u001b[39mGREEN\u001b[39m}\u001b[39;00m\u001b[39mEPOCH \u001b[39m\u001b[39m{\u001b[39;00mepoch_number\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m{\u001b[39;00mStyle\u001b[39m.\u001b[39mRESET_ALL\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=225'>226</a>\u001b[0m \u001b[39m# Train for one epoch\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=226'>227</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_one_epoch(train_loader)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=228'>229</a>\u001b[0m \u001b[39m# Validate on the validation subset\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=229'>230</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mFore\u001b[39m.\u001b[39mCYAN\u001b[39m}\u001b[39;00m\u001b[39mValidation :\u001b[39m\u001b[39m{\u001b[39;00mStyle\u001b[39m.\u001b[39mRESET_ALL\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\model.py\u001b[0m in \u001b[0;36mline 142\u001b[0m, in \u001b[0;36mModel.train_one_epoch\u001b[1;34m(self, dataloader)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=138'>139</a>\u001b[0m \u001b[39m# loss_function = torch.nn.CrossEntropyLoss()\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=140'>141</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=141'>142</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(dataloader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=143'>144</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=145'>146</a>\u001b[0m             optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1174'>1175</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1176'>1177</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1177'>1178</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1178'>1179</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1179'>1180</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1180'>1181</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=626'>627</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=627'>628</a>\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=628'>629</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=629'>630</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=630'>631</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=631'>632</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=632'>633</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=633'>634</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=671'>672</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=672'>673</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=673'>674</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=674'>675</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=675'>676</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=52'>53</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=52'>53</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\data_processing.py:47\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=45'>46</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, i):\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m : get_audio(i, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath),\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=47'>48</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m : get_text(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_data),\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=48'>49</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m : get_label(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_data)}\n",
      "File \u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\data_processing.py:20\u001b[0m, in \u001b[0;36mget_audio\u001b[1;34m(i, raw_data, filepath)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=17'>18</a>\u001b[0m audio_file_path \u001b[39m=\u001b[39m  filepath \u001b[39m+\u001b[39m raw_data[\u001b[39m\"\u001b[39m\u001b[39mdyad\u001b[39m\u001b[39m\"\u001b[39m][i]\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mtranscr\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.wav\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=18'>19</a>\u001b[0m audio_tensor, sampling_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mload(audio_file_path)\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=19'>20</a>\u001b[0m audio_tensor \u001b[39m=\u001b[39m processor(audio_tensor, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, sampling_rate \u001b[39m=\u001b[39;49m sampling_rate)\u001b[39m.\u001b[39minput_values\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=20'>21</a>\u001b[0m stop \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(sampling_rate\u001b[39m*\u001b[39mraw_data[\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m][i])\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=21'>22</a>\u001b[0m start \u001b[39m=\u001b[39m stop\u001b[39m-\u001b[39msampling_rate\u001b[39m*\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:94\u001b[0m, in \u001b[0;36mWav2Vec2Processor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=90'>91</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou need to specify either an `audio` or `text` input to process.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=92'>93</a>\u001b[0m \u001b[39mif\u001b[39;00m audio \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=93'>94</a>\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_extractor(audio, \u001b[39m*\u001b[39;49margs, sampling_rate\u001b[39m=\u001b[39;49msampling_rate, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=94'>95</a>\u001b[0m \u001b[39mif\u001b[39;00m text \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=95'>96</a>\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(text, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py:233\u001b[0m, in \u001b[0;36mWav2Vec2FeatureExtractor.__call__\u001b[1;34m(self, raw_speech, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors, sampling_rate, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=226'>227</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_normalize:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=227'>228</a>\u001b[0m     attention_mask \u001b[39m=\u001b[39m (\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=228'>229</a>\u001b[0m         attention_mask\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=229'>230</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_strategies(padding, max_length\u001b[39m=\u001b[39mmax_length) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=230'>231</a>\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=231'>232</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=232'>233</a>\u001b[0m     padded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_values\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzero_mean_unit_var_norm(\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=233'>234</a>\u001b[0m         padded_inputs[\u001b[39m\"\u001b[39;49m\u001b[39minput_values\u001b[39;49m\u001b[39m\"\u001b[39;49m], attention_mask\u001b[39m=\u001b[39;49mattention_mask, padding_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_value\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=234'>235</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=236'>237</a>\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=237'>238</a>\u001b[0m     padded_inputs \u001b[39m=\u001b[39m padded_inputs\u001b[39m.\u001b[39mconvert_to_tensors(return_tensors)\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py:98\u001b[0m, in \u001b[0;36mWav2Vec2FeatureExtractor.zero_mean_unit_var_norm\u001b[1;34m(input_values, attention_mask, padding_value)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=95'>96</a>\u001b[0m         normed_input_values\u001b[39m.\u001b[39mappend(normed_slice)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=96'>97</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=97'>98</a>\u001b[0m     normed_input_values \u001b[39m=\u001b[39m [(x \u001b[39m-\u001b[39;49m x\u001b[39m.\u001b[39;49mmean()) \u001b[39m/\u001b[39;49m np\u001b[39m.\u001b[39;49msqrt(x\u001b[39m.\u001b[39;49mvar() \u001b[39m+\u001b[39;49m \u001b[39m1e-7\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m input_values]\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=99'>100</a>\u001b[0m \u001b[39mreturn\u001b[39;00m normed_input_values\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py:98\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=95'>96</a>\u001b[0m         normed_input_values\u001b[39m.\u001b[39mappend(normed_slice)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=96'>97</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=97'>98</a>\u001b[0m     normed_input_values \u001b[39m=\u001b[39m [(x \u001b[39m-\u001b[39m x\u001b[39m.\u001b[39mmean()) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msqrt(x\u001b[39m.\u001b[39;49mvar() \u001b[39m+\u001b[39m \u001b[39m1e-7\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m input_values]\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=99'>100</a>\u001b[0m \u001b[39mreturn\u001b[39;00m normed_input_values\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:247\u001b[0m, in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=241'>242</a>\u001b[0m \u001b[39m# Most general case; includes handling object arrays containing imaginary\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=242'>243</a>\u001b[0m \u001b[39m# numbers and complex types with non-native byteorder\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=243'>244</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=244'>245</a>\u001b[0m     x \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39mmultiply(x, um\u001b[39m.\u001b[39mconjugate(x), out\u001b[39m=\u001b[39mx)\u001b[39m.\u001b[39mreal\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=246'>247</a>\u001b[0m ret \u001b[39m=\u001b[39m umr_sum(x, axis, dtype, out, keepdims\u001b[39m=\u001b[39mkeepdims, where\u001b[39m=\u001b[39mwhere)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=248'>249</a>\u001b[0m \u001b[39m# Compute degrees of freedom and make sure it is not negative.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=249'>250</a>\u001b[0m rcount \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39mmaximum(rcount \u001b[39m-\u001b[39m ddof, \u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style\n",
    "from data_processing import *\n",
    "from transformers import Wav2Vec2Model\n",
    "from transformers import CamembertModel\n",
    "from icecream import ic\n",
    "\n",
    "########################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "wave2vec_name = \"facebook/wav2vec2-base-960h\"\n",
    "wave2vec_model = Wav2Vec2Model.from_pretrained(wave2vec_name)\n",
    "wave2vec_model = wave2vec_model.to(device)\n",
    "\n",
    "bert_name = 'camembert-base'\n",
    "bert_model = CamembertModel.from_pretrained(bert_name)\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "for param in wave2vec_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "########################################################################\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model using covolutional neural net architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, device):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define model components\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "###################\n",
    "        \n",
    "    def parameters_number(self):\n",
    "        return(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Define the forward pass using the model components\n",
    "        \n",
    "        input_data['audio'] = input_data['audio'].to(self.device)\n",
    "        input_data['text'] = input_data['text'].to(self.device)\n",
    "        input_data['label'] = input_data['label'].to(self.device)\n",
    "\n",
    "        channel0 = input_data['audio'][:,0,:]\n",
    "        channel1 = input_data['audio'][:,1,:]\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_model(channel0)\n",
    "        wave2vec_output1 = wave2vec_model(channel1)\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_output0.last_hidden_state\n",
    "        wave2vec_output1 = wave2vec_output1.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.nn.functional.max_pool1d(wave2vec_output0, kernel_size=6)\n",
    "        wave2vec_output1 = torch.nn.functional.max_pool1d(wave2vec_output1, kernel_size=6)\n",
    "        \n",
    "        bert_output = bert_model(input_data['text'])\n",
    "        bert_output = bert_output.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.flatten(wave2vec_output0, start_dim=1)\n",
    "        wave2vec_output1 = torch.flatten(wave2vec_output1, start_dim=1)\n",
    "        bert_output = torch.flatten(bert_output, start_dim=1)\n",
    "\n",
    "        # Concatenate or combine the outputs as needed\n",
    "        combined_output = torch.cat((wave2vec_output0, wave2vec_output1, bert_output), dim=1)\n",
    "\n",
    "        # Apply linear layers\n",
    "        linear1_output = torch.relu(self.linear1(combined_output))\n",
    "        final_output = self.linear2(self.dropout(linear1_output))\n",
    "\n",
    "        return torch.softmax(final_output, dim=1)\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        self.eval()\n",
    "\n",
    "        # Define the loss function\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        true_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_negative_1 = torch.tensor(0).to(self.device)\n",
    "        \n",
    "        true_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_negative_0 = torch.tensor(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Validating\")):\n",
    "                \n",
    "                output = self.forward(batch)\n",
    "                labels = batch[\"label\"].long()\n",
    "\n",
    "                loss = loss_function(output, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                \n",
    "                true_positive_1 += torch.sum((predicted == labels) * (labels == 1))\n",
    "                false_positive_1 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 1))\n",
    "                false_negative_1 += torch.sum(((1-predicted) == labels) * (labels == 1))\n",
    "                \n",
    "                true_positive_0 += torch.sum((predicted == labels) * (labels == 0))\n",
    "                false_positive_0 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 0))\n",
    "                false_negative_0 += torch.sum(((1-predicted) == labels) * (labels == 0))\n",
    "                \n",
    "        precision_1 = true_positive_1 / max((true_positive_1 + false_positive_1), 1)\n",
    "        recall_1 = true_positive_1 / max((true_positive_1 + false_negative_1), 1) \n",
    "        \n",
    "        precision_0 = true_positive_0 / max((true_positive_0 + false_positive_0), 1)\n",
    "        recall_0 = true_positive_0 / max((true_positive_0 + false_negative_0), 1) \n",
    "        \n",
    "        f1_1 = 2 * (precision_1 * recall_1) / max((precision_1 + recall_1), 1)\n",
    "        f1_0 = 2 * (precision_0 * recall_0) / max((precision_0 + recall_0), 1)\n",
    "        \n",
    "        print(f\"Classe {Fore.RED}0{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_0 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_0 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_0 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Classe {Fore.RED}1{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_1 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_1 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_1 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Score : {(f1_0*0.18 + f1_1*(1-0.18))}\")\n",
    "                        \n",
    "    def train_one_epoch(self, dataloader):\n",
    "        \n",
    "        self.train(True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        # loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        try:\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "                \n",
    "                try:\n",
    "                \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    output = self.forward(batch)\n",
    "                    labels = batch[\"label\"].long()\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    \n",
    "                    true_positive_0 = (output * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "                    false_positive_0 = ((1 - output) * labels.unsqueeze(1))[:, 0].sum()\n",
    "                    false_negative_0 = ((1 - output) * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "\n",
    "                    true_positive_1 = (output * labels.unsqueeze(1))[:, 1].sum()\n",
    "                    false_positive_1 = (output * (1 - labels).unsqueeze(1))[:, 1].sum()\n",
    "                    false_negative_1 = ((1 - output) * labels.unsqueeze(1))[:, 1].sum()\n",
    "\n",
    "                    epsilon = torch.tensor([1e-7]).to(self.device) # 1e-7\n",
    "                    \n",
    "                    precision_0 = true_positive_0 / (true_positive_0 + false_positive_0 + epsilon)\n",
    "                    recall_0 = true_positive_0 / (true_positive_0 + false_negative_0 + epsilon)\n",
    "                    f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0 + epsilon)\n",
    "\n",
    "                    precision_1 = true_positive_1 / (true_positive_1 + false_positive_1 + epsilon)\n",
    "                    recall_1 = true_positive_1 / (true_positive_1 + false_negative_1 + epsilon)\n",
    "                    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1 + epsilon)\n",
    "                \n",
    "                    loss = -torch.log((f1_0*0.18 + f1_1*(1-0.18)) + epsilon)\n",
    "                    \n",
    "                    # ic(output, labels)\n",
    "                    # ic(true_positive_0, false_positive_0, false_negative_0, true_positive_1, false_positive_1, false_negative_1)\n",
    "                    # ic(precision_0, recall_0, f1_0, precision_1, recall_1, f1_1)\n",
    "                    # print(\"Successfully trained a batch\")\n",
    "                    \n",
    "                    ic(loss)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # labels = batch[\"label\"].long()\n",
    "                    # loss = loss_function(output, labels)\n",
    "                    # loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                except StopIteration as e:\n",
    "                    print(f\"An error occurred: {e}\")   \n",
    "                    # Catch StopIteration and continue to the next batch\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            # Handle other exceptions if needed\n",
    "            print(f\"An error occurred: {e}\")      \n",
    "\n",
    "    def train_loop(self, generator, nb_epoch):\n",
    "        \n",
    "        self.to(self.device)\n",
    "\n",
    "        data = generator.raw_data\n",
    "\n",
    "        train_data = data.sample(frac=0.9,random_state=200)\n",
    "        test_data = data.drop(train_data.index)\n",
    "\n",
    "        test_data.reset_index(drop=True, inplace=True)\n",
    "        train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_generator = DataGenerator(test_data, filepath)\n",
    "        test_loader = create_dataloader(test_generator)\n",
    "        \n",
    "        for epoch_number in range(nb_epoch):\n",
    "\n",
    "            train_subdata = train_data.sample(frac=0.9,random_state=200)\n",
    "            val_subdata = train_data.drop(train_subdata.index)\n",
    "            \n",
    "            train_subdata.reset_index(drop=True, inplace=True)\n",
    "            val_subdata.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            train_subgenerator = DataGenerator(train_subdata, filepath)\n",
    "            val_subgenerator = DataGenerator(val_subdata, filepath)\n",
    "\n",
    "            train_loader = create_dataloader(train_subgenerator)\n",
    "            val_loader = create_dataloader(val_subgenerator)\n",
    "\n",
    "            print(\"\")    \n",
    "            print(f'{Fore.GREEN}EPOCH {epoch_number + 1}:{Style.RESET_ALL}')\n",
    "            \n",
    "            # Train for one epoch\n",
    "            self.train_one_epoch(train_loader)\n",
    "\n",
    "            # Validate on the validation subset\n",
    "            print(f'{Fore.CYAN}Validation :{Style.RESET_ALL}')\n",
    "            self.evaluate(val_loader)\n",
    "            print(f'{Fore.YELLOW}Test :{Style.RESET_ALL}')\n",
    "            self.evaluate(test_loader)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "generator = DataGenerator(raw_data.iloc[2048:2048*3], filepath)\n",
    "model = Model(27904, 16, 2, device)\n",
    "\n",
    "dataloader = create_dataloader(generator)\n",
    "model.evaluate(dataloader)\n",
    "\n",
    "# print(\"Nombre de paramètres du model:\", model.parameters_number())\n",
    "\n",
    "# model.train_loop(generator, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   0%|          | 0/64 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "1460",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:345\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/indexes/range.py?line=343'>344</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/indexes/range.py?line=344'>345</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_range\u001b[39m.\u001b[39mindex(new_key)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/indexes/range.py?line=345'>346</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[1;31mValueError\u001b[0m: 1460 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\model.py\u001b[0m in \u001b[0;36mline 241\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=237'>238</a>\u001b[0m model \u001b[39m=\u001b[39m Model(\u001b[39m27904\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m2\u001b[39m, device)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=239'>240</a>\u001b[0m dataloader \u001b[39m=\u001b[39m create_dataloader(generator)\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=240'>241</a>\u001b[0m model\u001b[39m.\u001b[39;49mevaluate(dataloader)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=242'>243</a>\u001b[0m \u001b[39m# print(\"Nombre de paramètres du model:\", model.parameters_number())\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=243'>244</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=244'>245</a>\u001b[0m \u001b[39m# model.train_loop(generator, 5)\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\model.py\u001b[0m in \u001b[0;36mline 103\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, dataloader)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=99'>100</a>\u001b[0m false_negative_0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=101'>102</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# Disable gradient computation during validation\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=102'>103</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(dataloader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidating\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=104'>105</a>\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(batch)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=105'>106</a>\u001b[0m         labels \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mlong()\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1174'>1175</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1176'>1177</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1177'>1178</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1178'>1179</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1179'>1180</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1180'>1181</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=626'>627</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=627'>628</a>\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=628'>629</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=629'>630</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=630'>631</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=631'>632</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=632'>633</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=633'>634</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=671'>672</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=672'>673</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=673'>674</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=674'>675</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=675'>676</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=52'>53</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=52'>53</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\data_processing.py:47\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=45'>46</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, i):\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m : get_audio(i, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath),\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=47'>48</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m : get_text(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_data),\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=48'>49</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m : get_label(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_data)}\n",
      "File \u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\data_processing.py:18\u001b[0m, in \u001b[0;36mget_audio\u001b[1;34m(i, raw_data, filepath)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_audio\u001b[39m(i, raw_data, filepath):\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=17'>18</a>\u001b[0m     audio_file_path \u001b[39m=\u001b[39m  filepath \u001b[39m+\u001b[39m raw_data[\u001b[39m\"\u001b[39;49m\u001b[39mdyad\u001b[39;49m\u001b[39m\"\u001b[39;49m][i]\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mtranscr\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.wav\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=18'>19</a>\u001b[0m     audio_tensor, sampling_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mload(audio_file_path)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=19'>20</a>\u001b[0m     audio_tensor \u001b[39m=\u001b[39m processor(audio_tensor, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, sampling_rate \u001b[39m=\u001b[39m sampling_rate)\u001b[39m.\u001b[39minput_values\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1012\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/series.py?line=1008'>1009</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/series.py?line=1010'>1011</a>\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/series.py?line=1011'>1012</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/series.py?line=1013'>1014</a>\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/series.py?line=1014'>1015</a>\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/series.py?line=1015'>1016</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/series.py?line=1016'>1017</a>\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/series.py?line=1117'>1118</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/series.py?line=1119'>1120</a>\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/series.py?line=1120'>1121</a>\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/series.py?line=1122'>1123</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/series.py?line=1123'>1124</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:347\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/indexes/range.py?line=344'>345</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_range\u001b[39m.\u001b[39mindex(new_key)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/indexes/range.py?line=345'>346</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/indexes/range.py?line=346'>347</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/indexes/range.py?line=347'>348</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/pandas/core/indexes/range.py?line=348'>349</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 1460"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style\n",
    "from data_processing import *\n",
    "from transformers import Wav2Vec2Model\n",
    "from transformers import CamembertModel\n",
    "from icecream import ic\n",
    "\n",
    "########################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "wave2vec_name = \"facebook/wav2vec2-base-960h\"\n",
    "wave2vec_model = Wav2Vec2Model.from_pretrained(wave2vec_name)\n",
    "wave2vec_model = wave2vec_model.to(device)\n",
    "\n",
    "bert_name = 'camembert-base'\n",
    "bert_model = CamembertModel.from_pretrained(bert_name)\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "for param in wave2vec_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "########################################################################\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model using covolutional neural net architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, device):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define model components\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "###################\n",
    "        \n",
    "    def parameters_number(self):\n",
    "        return(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Define the forward pass using the model components\n",
    "        \n",
    "        input_data['audio'] = input_data['audio'].to(self.device)\n",
    "        input_data['text'] = input_data['text'].to(self.device)\n",
    "        input_data['label'] = input_data['label'].to(self.device)\n",
    "\n",
    "        channel0 = input_data['audio'][:,0,:]\n",
    "        channel1 = input_data['audio'][:,1,:]\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_model(channel0)\n",
    "        wave2vec_output1 = wave2vec_model(channel1)\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_output0.last_hidden_state\n",
    "        wave2vec_output1 = wave2vec_output1.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.nn.functional.max_pool1d(wave2vec_output0, kernel_size=6)\n",
    "        wave2vec_output1 = torch.nn.functional.max_pool1d(wave2vec_output1, kernel_size=6)\n",
    "        \n",
    "        bert_output = bert_model(input_data['text'])\n",
    "        bert_output = bert_output.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.flatten(wave2vec_output0, start_dim=1)\n",
    "        wave2vec_output1 = torch.flatten(wave2vec_output1, start_dim=1)\n",
    "        bert_output = torch.flatten(bert_output, start_dim=1)\n",
    "\n",
    "        # Concatenate or combine the outputs as needed\n",
    "        combined_output = torch.cat((wave2vec_output0, wave2vec_output1, bert_output), dim=1)\n",
    "\n",
    "        # Apply linear layers\n",
    "        linear1_output = torch.relu(self.linear1(combined_output))\n",
    "        final_output = self.linear2(self.dropout(linear1_output))\n",
    "\n",
    "        return torch.softmax(final_output, dim=1)\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        self.eval()\n",
    "\n",
    "        # Define the loss function\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        true_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_negative_1 = torch.tensor(0).to(self.device)\n",
    "        \n",
    "        true_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_negative_0 = torch.tensor(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Validating\")):\n",
    "                \n",
    "                output = self.forward(batch)\n",
    "                labels = batch[\"label\"].long()\n",
    "\n",
    "                loss = loss_function(output, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                \n",
    "                true_positive_1 += torch.sum((predicted == labels) * (labels == 1))\n",
    "                false_positive_1 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 1))\n",
    "                false_negative_1 += torch.sum(((1-predicted) == labels) * (labels == 1))\n",
    "                \n",
    "                true_positive_0 += torch.sum((predicted == labels) * (labels == 0))\n",
    "                false_positive_0 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 0))\n",
    "                false_negative_0 += torch.sum(((1-predicted) == labels) * (labels == 0))\n",
    "                \n",
    "        precision_1 = true_positive_1 / max((true_positive_1 + false_positive_1), 1)\n",
    "        recall_1 = true_positive_1 / max((true_positive_1 + false_negative_1), 1) \n",
    "        \n",
    "        precision_0 = true_positive_0 / max((true_positive_0 + false_positive_0), 1)\n",
    "        recall_0 = true_positive_0 / max((true_positive_0 + false_negative_0), 1) \n",
    "        \n",
    "        f1_1 = 2 * (precision_1 * recall_1) / max((precision_1 + recall_1), 1)\n",
    "        f1_0 = 2 * (precision_0 * recall_0) / max((precision_0 + recall_0), 1)\n",
    "        \n",
    "        print(f\"Classe {Fore.RED}0{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_0 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_0 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_0 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Classe {Fore.RED}1{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_1 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_1 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_1 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Score : {(f1_0*0.18 + f1_1*(1-0.18))}\")\n",
    "                        \n",
    "    def train_one_epoch(self, dataloader):\n",
    "        \n",
    "        self.train(True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        # loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        try:\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "                \n",
    "                try:\n",
    "                \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    output = self.forward(batch)\n",
    "                    labels = batch[\"label\"].long()\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    \n",
    "                    true_positive_0 = (output * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "                    false_positive_0 = ((1 - output) * labels.unsqueeze(1))[:, 0].sum()\n",
    "                    false_negative_0 = ((1 - output) * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "\n",
    "                    true_positive_1 = (output * labels.unsqueeze(1))[:, 1].sum()\n",
    "                    false_positive_1 = (output * (1 - labels).unsqueeze(1))[:, 1].sum()\n",
    "                    false_negative_1 = ((1 - output) * labels.unsqueeze(1))[:, 1].sum()\n",
    "\n",
    "                    epsilon = torch.tensor([1e-7]).to(self.device) # 1e-7\n",
    "                    \n",
    "                    precision_0 = true_positive_0 / (true_positive_0 + false_positive_0 + epsilon)\n",
    "                    recall_0 = true_positive_0 / (true_positive_0 + false_negative_0 + epsilon)\n",
    "                    f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0 + epsilon)\n",
    "\n",
    "                    precision_1 = true_positive_1 / (true_positive_1 + false_positive_1 + epsilon)\n",
    "                    recall_1 = true_positive_1 / (true_positive_1 + false_negative_1 + epsilon)\n",
    "                    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1 + epsilon)\n",
    "                \n",
    "                    loss = -torch.log((f1_0*0.18 + f1_1*(1-0.18)) + epsilon)\n",
    "                    \n",
    "                    # ic(output, labels)\n",
    "                    # ic(true_positive_0, false_positive_0, false_negative_0, true_positive_1, false_positive_1, false_negative_1)\n",
    "                    # ic(precision_0, recall_0, f1_0, precision_1, recall_1, f1_1)\n",
    "                    # print(\"Successfully trained a batch\")\n",
    "                    \n",
    "                    ic(loss)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # labels = batch[\"label\"].long()\n",
    "                    # loss = loss_function(output, labels)\n",
    "                    # loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                except StopIteration as e:\n",
    "                    print(f\"An error occurred: {e}\")   \n",
    "                    # Catch StopIteration and continue to the next batch\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            # Handle other exceptions if needed\n",
    "            print(f\"An error occurred: {e}\")      \n",
    "\n",
    "    def train_loop(self, generator, nb_epoch):\n",
    "        \n",
    "        self.to(self.device)\n",
    "\n",
    "        data = generator.raw_data\n",
    "\n",
    "        train_data = data.sample(frac=0.9,random_state=200)\n",
    "        test_data = data.drop(train_data.index)\n",
    "\n",
    "        test_data.reset_index(drop=True, inplace=True)\n",
    "        train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_generator = DataGenerator(test_data, filepath)\n",
    "        test_loader = create_dataloader(test_generator)\n",
    "        \n",
    "        for epoch_number in range(nb_epoch):\n",
    "\n",
    "            train_subdata = train_data.sample(frac=0.9,random_state=200)\n",
    "            val_subdata = train_data.drop(train_subdata.index)\n",
    "            \n",
    "            train_subdata.reset_index(drop=True, inplace=True)\n",
    "            val_subdata.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            train_subgenerator = DataGenerator(train_subdata, filepath)\n",
    "            val_subgenerator = DataGenerator(val_subdata, filepath)\n",
    "\n",
    "            train_loader = create_dataloader(train_subgenerator)\n",
    "            val_loader = create_dataloader(val_subgenerator)\n",
    "\n",
    "            print(\"\")    \n",
    "            print(f'{Fore.GREEN}EPOCH {epoch_number + 1}:{Style.RESET_ALL}')\n",
    "            \n",
    "            # Train for one epoch\n",
    "            self.train_one_epoch(train_loader)\n",
    "\n",
    "            # Validate on the validation subset\n",
    "            print(f'{Fore.CYAN}Validation :{Style.RESET_ALL}')\n",
    "            self.evaluate(val_loader)\n",
    "            print(f'{Fore.YELLOW}Test :{Style.RESET_ALL}')\n",
    "            self.evaluate(test_loader)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "generator = DataGenerator(raw_data.iloc[:64], filepath)\n",
    "model = Model(27904, 16, 2, device)\n",
    "\n",
    "dataloader = create_dataloader(generator)\n",
    "model.evaluate(dataloader)\n",
    "\n",
    "# print(\"Nombre de paramètres du model:\", model.parameters_number())\n",
    "\n",
    "# model.train_loop(generator, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   0%|          | 0/1 [00:27<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\model.py\u001b[0m in \u001b[0;36mline 241\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=237'>238</a>\u001b[0m model \u001b[39m=\u001b[39m Model(\u001b[39m27904\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m2\u001b[39m, device)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=239'>240</a>\u001b[0m dataloader \u001b[39m=\u001b[39m create_dataloader(generator)\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=240'>241</a>\u001b[0m model\u001b[39m.\u001b[39;49mevaluate(dataloader)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=242'>243</a>\u001b[0m \u001b[39m# print(\"Nombre de paramètres du model:\", model.parameters_number())\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=243'>244</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=244'>245</a>\u001b[0m \u001b[39m# model.train_loop(generator, 5)\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\model.py\u001b[0m in \u001b[0;36mline 105\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, dataloader)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=101'>102</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# Disable gradient computation during validation\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=102'>103</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(dataloader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidating\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=104'>105</a>\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(batch)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=105'>106</a>\u001b[0m         labels \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mlong()\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=107'>108</a>\u001b[0m         loss \u001b[39m=\u001b[39m loss_function(output, labels)\n",
      "\u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\model.py\u001b[0m in \u001b[0;36mline 82\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, input_data)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=78'>79</a>\u001b[0m combined_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((wave2vec_output0, wave2vec_output1, bert_output), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=80'>81</a>\u001b[0m \u001b[39m# Apply linear layers\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=81'>82</a>\u001b[0m linear1_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(combined_output))\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=82'>83</a>\u001b[0m final_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(linear1_output))\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=84'>85</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msoftmax(final_output, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/nn/modules/linear.py?line=112'>113</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/nn/modules/linear.py?line=113'>114</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style\n",
    "from data_processing import *\n",
    "from transformers import Wav2Vec2Model\n",
    "from transformers import CamembertModel\n",
    "from icecream import ic\n",
    "\n",
    "########################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "wave2vec_name = \"facebook/wav2vec2-base-960h\"\n",
    "wave2vec_model = Wav2Vec2Model.from_pretrained(wave2vec_name)\n",
    "wave2vec_model = wave2vec_model.to(device)\n",
    "\n",
    "bert_name = 'camembert-base'\n",
    "bert_model = CamembertModel.from_pretrained(bert_name)\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "for param in wave2vec_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "########################################################################\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model using covolutional neural net architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, device):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define model components\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "###################\n",
    "        \n",
    "    def parameters_number(self):\n",
    "        return(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Define the forward pass using the model components\n",
    "        \n",
    "        input_data['audio'] = input_data['audio'].to(self.device)\n",
    "        input_data['text'] = input_data['text'].to(self.device)\n",
    "        input_data['label'] = input_data['label'].to(self.device)\n",
    "\n",
    "        channel0 = input_data['audio'][:,0,:]\n",
    "        channel1 = input_data['audio'][:,1,:]\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_model(channel0)\n",
    "        wave2vec_output1 = wave2vec_model(channel1)\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_output0.last_hidden_state\n",
    "        wave2vec_output1 = wave2vec_output1.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.nn.functional.max_pool1d(wave2vec_output0, kernel_size=6)\n",
    "        wave2vec_output1 = torch.nn.functional.max_pool1d(wave2vec_output1, kernel_size=6)\n",
    "        \n",
    "        bert_output = bert_model(input_data['text'])\n",
    "        bert_output = bert_output.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.flatten(wave2vec_output0, start_dim=1)\n",
    "        wave2vec_output1 = torch.flatten(wave2vec_output1, start_dim=1)\n",
    "        bert_output = torch.flatten(bert_output, start_dim=1)\n",
    "\n",
    "        # Concatenate or combine the outputs as needed\n",
    "        combined_output = torch.cat((wave2vec_output0, wave2vec_output1, bert_output), dim=1)\n",
    "\n",
    "        # Apply linear layers\n",
    "        linear1_output = torch.relu(self.linear1(combined_output))\n",
    "        final_output = self.linear2(self.dropout(linear1_output))\n",
    "\n",
    "        return torch.softmax(final_output, dim=1)\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.eval()\n",
    "\n",
    "        # Define the loss function\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        true_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_negative_1 = torch.tensor(0).to(self.device)\n",
    "        \n",
    "        true_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_negative_0 = torch.tensor(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Validating\")):\n",
    "                \n",
    "                output = self.forward(batch)\n",
    "                labels = batch[\"label\"].long()\n",
    "\n",
    "                loss = loss_function(output, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                \n",
    "                true_positive_1 += torch.sum((predicted == labels) * (labels == 1))\n",
    "                false_positive_1 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 1))\n",
    "                false_negative_1 += torch.sum(((1-predicted) == labels) * (labels == 1))\n",
    "                \n",
    "                true_positive_0 += torch.sum((predicted == labels) * (labels == 0))\n",
    "                false_positive_0 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 0))\n",
    "                false_negative_0 += torch.sum(((1-predicted) == labels) * (labels == 0))\n",
    "                \n",
    "        precision_1 = true_positive_1 / max((true_positive_1 + false_positive_1), 1)\n",
    "        recall_1 = true_positive_1 / max((true_positive_1 + false_negative_1), 1) \n",
    "        \n",
    "        precision_0 = true_positive_0 / max((true_positive_0 + false_positive_0), 1)\n",
    "        recall_0 = true_positive_0 / max((true_positive_0 + false_negative_0), 1) \n",
    "        \n",
    "        f1_1 = 2 * (precision_1 * recall_1) / max((precision_1 + recall_1), 1)\n",
    "        f1_0 = 2 * (precision_0 * recall_0) / max((precision_0 + recall_0), 1)\n",
    "        \n",
    "        print(f\"Classe {Fore.RED}0{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_0 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_0 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_0 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Classe {Fore.RED}1{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_1 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_1 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_1 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Score : {(f1_0*0.18 + f1_1*(1-0.18))}\")\n",
    "                        \n",
    "    def train_one_epoch(self, dataloader):\n",
    "        \n",
    "        self.train(True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        # loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        try:\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "                \n",
    "                try:\n",
    "                \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    output = self.forward(batch)\n",
    "                    labels = batch[\"label\"].long()\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    \n",
    "                    true_positive_0 = (output * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "                    false_positive_0 = ((1 - output) * labels.unsqueeze(1))[:, 0].sum()\n",
    "                    false_negative_0 = ((1 - output) * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "\n",
    "                    true_positive_1 = (output * labels.unsqueeze(1))[:, 1].sum()\n",
    "                    false_positive_1 = (output * (1 - labels).unsqueeze(1))[:, 1].sum()\n",
    "                    false_negative_1 = ((1 - output) * labels.unsqueeze(1))[:, 1].sum()\n",
    "\n",
    "                    epsilon = torch.tensor([1e-7]).to(self.device) # 1e-7\n",
    "                    \n",
    "                    precision_0 = true_positive_0 / (true_positive_0 + false_positive_0 + epsilon)\n",
    "                    recall_0 = true_positive_0 / (true_positive_0 + false_negative_0 + epsilon)\n",
    "                    f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0 + epsilon)\n",
    "\n",
    "                    precision_1 = true_positive_1 / (true_positive_1 + false_positive_1 + epsilon)\n",
    "                    recall_1 = true_positive_1 / (true_positive_1 + false_negative_1 + epsilon)\n",
    "                    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1 + epsilon)\n",
    "                \n",
    "                    loss = -torch.log((f1_0*0.18 + f1_1*(1-0.18)) + epsilon)\n",
    "                    \n",
    "                    # ic(output, labels)\n",
    "                    # ic(true_positive_0, false_positive_0, false_negative_0, true_positive_1, false_positive_1, false_negative_1)\n",
    "                    # ic(precision_0, recall_0, f1_0, precision_1, recall_1, f1_1)\n",
    "                    # print(\"Successfully trained a batch\")\n",
    "                    \n",
    "                    ic(loss)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # labels = batch[\"label\"].long()\n",
    "                    # loss = loss_function(output, labels)\n",
    "                    # loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                except StopIteration as e:\n",
    "                    print(f\"An error occurred: {e}\")   \n",
    "                    # Catch StopIteration and continue to the next batch\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            # Handle other exceptions if needed\n",
    "            print(f\"An error occurred: {e}\")      \n",
    "\n",
    "    def train_loop(self, generator, nb_epoch):\n",
    "        \n",
    "        self.to(self.device)\n",
    "\n",
    "        data = generator.raw_data\n",
    "\n",
    "        train_data = data.sample(frac=0.9,random_state=200)\n",
    "        test_data = data.drop(train_data.index)\n",
    "\n",
    "        test_data.reset_index(drop=True, inplace=True)\n",
    "        train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_generator = DataGenerator(test_data, filepath)\n",
    "        test_loader = create_dataloader(test_generator)\n",
    "        \n",
    "        for epoch_number in range(nb_epoch):\n",
    "\n",
    "            train_subdata = train_data.sample(frac=0.9,random_state=200)\n",
    "            val_subdata = train_data.drop(train_subdata.index)\n",
    "            \n",
    "            train_subdata.reset_index(drop=True, inplace=True)\n",
    "            val_subdata.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            train_subgenerator = DataGenerator(train_subdata, filepath)\n",
    "            val_subgenerator = DataGenerator(val_subdata, filepath)\n",
    "\n",
    "            train_loader = create_dataloader(train_subgenerator)\n",
    "            val_loader = create_dataloader(val_subgenerator)\n",
    "\n",
    "            print(\"\")    \n",
    "            print(f'{Fore.GREEN}EPOCH {epoch_number + 1}:{Style.RESET_ALL}')\n",
    "            \n",
    "            # Train for one epoch\n",
    "            self.train_one_epoch(train_loader)\n",
    "\n",
    "            # Validate on the validation subset\n",
    "            print(f'{Fore.CYAN}Validation :{Style.RESET_ALL}')\n",
    "            self.evaluate(val_loader)\n",
    "            print(f'{Fore.YELLOW}Test :{Style.RESET_ALL}')\n",
    "            self.evaluate(test_loader)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "generator = DataGenerator(raw_data.iloc[:64], filepath)\n",
    "model = Model(27904, 16, 2, device)\n",
    "\n",
    "dataloader = create_dataloader(generator)\n",
    "model.evaluate(dataloader)\n",
    "\n",
    "# print(\"Nombre de paramètres du model:\", model.parameters_number())\n",
    "\n",
    "# model.train_loop(generator, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████| 1/1 [00:23<00:00, 23.04s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m0.00%\u001b[0m, Recall: \u001b[32m0.00%\u001b[0m, F1 Score: \u001b[32m0.00%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m15.62%\u001b[0m, Recall: \u001b[32m100.00%\u001b[0m, F1 Score: \u001b[32m27.03%\u001b[0m\n",
      "Score : 0.22162160277366638\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style\n",
    "from data_processing import *\n",
    "from transformers import Wav2Vec2Model\n",
    "from transformers import CamembertModel\n",
    "from icecream import ic\n",
    "\n",
    "########################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "wave2vec_name = \"facebook/wav2vec2-base-960h\"\n",
    "wave2vec_model = Wav2Vec2Model.from_pretrained(wave2vec_name)\n",
    "wave2vec_model = wave2vec_model.to(device)\n",
    "\n",
    "bert_name = 'camembert-base'\n",
    "bert_model = CamembertModel.from_pretrained(bert_name)\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "for param in wave2vec_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "########################################################################\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model using covolutional neural net architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, device):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define model components\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "###################\n",
    "        \n",
    "    def parameters_number(self):\n",
    "        return(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Define the forward pass using the model components\n",
    "        \n",
    "        input_data['audio'] = input_data['audio'].to(self.device)\n",
    "        input_data['text'] = input_data['text'].to(self.device)\n",
    "        input_data['label'] = input_data['label'].to(self.device)\n",
    "\n",
    "        channel0 = input_data['audio'][:,0,:]\n",
    "        channel1 = input_data['audio'][:,1,:]\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_model(channel0)\n",
    "        wave2vec_output1 = wave2vec_model(channel1)\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_output0.last_hidden_state\n",
    "        wave2vec_output1 = wave2vec_output1.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.nn.functional.max_pool1d(wave2vec_output0, kernel_size=6)\n",
    "        wave2vec_output1 = torch.nn.functional.max_pool1d(wave2vec_output1, kernel_size=6)\n",
    "        \n",
    "        bert_output = bert_model(input_data['text'])\n",
    "        bert_output = bert_output.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.flatten(wave2vec_output0, start_dim=1)\n",
    "        wave2vec_output1 = torch.flatten(wave2vec_output1, start_dim=1)\n",
    "        bert_output = torch.flatten(bert_output, start_dim=1)\n",
    "\n",
    "        # Concatenate or combine the outputs as needed\n",
    "        combined_output = torch.cat((wave2vec_output0, wave2vec_output1, bert_output), dim=1)\n",
    "\n",
    "        # Apply linear layers\n",
    "        linear1_output = torch.relu(self.linear1(combined_output))\n",
    "        final_output = self.linear2(self.dropout(linear1_output))\n",
    "\n",
    "        return torch.softmax(final_output, dim=1)\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.eval()\n",
    "\n",
    "        # Define the loss function\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        true_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_negative_1 = torch.tensor(0).to(self.device)\n",
    "        \n",
    "        true_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_negative_0 = torch.tensor(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Validating\")):\n",
    "                \n",
    "                output = self.forward(batch)\n",
    "                labels = batch[\"label\"].long()\n",
    "\n",
    "                loss = loss_function(output, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                \n",
    "                true_positive_1 += torch.sum((predicted == labels) * (labels == 1))\n",
    "                false_positive_1 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 1))\n",
    "                false_negative_1 += torch.sum(((1-predicted) == labels) * (labels == 1))\n",
    "                \n",
    "                true_positive_0 += torch.sum((predicted == labels) * (labels == 0))\n",
    "                false_positive_0 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 0))\n",
    "                false_negative_0 += torch.sum(((1-predicted) == labels) * (labels == 0))\n",
    "                \n",
    "        precision_1 = true_positive_1 / max((true_positive_1 + false_positive_1), 1)\n",
    "        recall_1 = true_positive_1 / max((true_positive_1 + false_negative_1), 1) \n",
    "        \n",
    "        precision_0 = true_positive_0 / max((true_positive_0 + false_positive_0), 1)\n",
    "        recall_0 = true_positive_0 / max((true_positive_0 + false_negative_0), 1) \n",
    "        \n",
    "        f1_1 = 2 * (precision_1 * recall_1) / max((precision_1 + recall_1), 1)\n",
    "        f1_0 = 2 * (precision_0 * recall_0) / max((precision_0 + recall_0), 1)\n",
    "        \n",
    "        print(f\"Classe {Fore.RED}0{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_0 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_0 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_0 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Classe {Fore.RED}1{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_1 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_1 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_1 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Score : {(f1_0*0.18 + f1_1*(1-0.18))}\")\n",
    "                        \n",
    "    def train_one_epoch(self, dataloader):\n",
    "        \n",
    "        self.train(True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        # loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        try:\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "                \n",
    "                try:\n",
    "                \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    output = self.forward(batch)\n",
    "                    labels = batch[\"label\"].long()\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    \n",
    "                    true_positive_0 = (output * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "                    false_positive_0 = ((1 - output) * labels.unsqueeze(1))[:, 0].sum()\n",
    "                    false_negative_0 = ((1 - output) * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "\n",
    "                    true_positive_1 = (output * labels.unsqueeze(1))[:, 1].sum()\n",
    "                    false_positive_1 = (output * (1 - labels).unsqueeze(1))[:, 1].sum()\n",
    "                    false_negative_1 = ((1 - output) * labels.unsqueeze(1))[:, 1].sum()\n",
    "\n",
    "                    epsilon = torch.tensor([1e-7]).to(self.device) # 1e-7\n",
    "                    \n",
    "                    precision_0 = true_positive_0 / (true_positive_0 + false_positive_0 + epsilon)\n",
    "                    recall_0 = true_positive_0 / (true_positive_0 + false_negative_0 + epsilon)\n",
    "                    f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0 + epsilon)\n",
    "\n",
    "                    precision_1 = true_positive_1 / (true_positive_1 + false_positive_1 + epsilon)\n",
    "                    recall_1 = true_positive_1 / (true_positive_1 + false_negative_1 + epsilon)\n",
    "                    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1 + epsilon)\n",
    "                \n",
    "                    loss = -torch.log((f1_0*0.18 + f1_1*(1-0.18)) + epsilon)\n",
    "                    \n",
    "                    # ic(output, labels)\n",
    "                    # ic(true_positive_0, false_positive_0, false_negative_0, true_positive_1, false_positive_1, false_negative_1)\n",
    "                    # ic(precision_0, recall_0, f1_0, precision_1, recall_1, f1_1)\n",
    "                    # print(\"Successfully trained a batch\")\n",
    "                    \n",
    "                    ic(loss)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # labels = batch[\"label\"].long()\n",
    "                    # loss = loss_function(output, labels)\n",
    "                    # loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                except StopIteration as e:\n",
    "                    print(f\"An error occurred: {e}\")   \n",
    "                    # Catch StopIteration and continue to the next batch\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            # Handle other exceptions if needed\n",
    "            print(f\"An error occurred: {e}\")      \n",
    "\n",
    "    def train_loop(self, generator, nb_epoch):\n",
    "        \n",
    "        self.to(self.device)\n",
    "\n",
    "        data = generator.raw_data\n",
    "\n",
    "        train_data = data.sample(frac=0.9,random_state=200)\n",
    "        test_data = data.drop(train_data.index)\n",
    "\n",
    "        test_data.reset_index(drop=True, inplace=True)\n",
    "        train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_generator = DataGenerator(test_data, filepath)\n",
    "        test_loader = create_dataloader(test_generator)\n",
    "        \n",
    "        for epoch_number in range(nb_epoch):\n",
    "\n",
    "            train_subdata = train_data.sample(frac=0.9,random_state=200)\n",
    "            val_subdata = train_data.drop(train_subdata.index)\n",
    "            \n",
    "            train_subdata.reset_index(drop=True, inplace=True)\n",
    "            val_subdata.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            train_subgenerator = DataGenerator(train_subdata, filepath)\n",
    "            val_subgenerator = DataGenerator(val_subdata, filepath)\n",
    "\n",
    "            train_loader = create_dataloader(train_subgenerator)\n",
    "            val_loader = create_dataloader(val_subgenerator)\n",
    "\n",
    "            print(\"\")    \n",
    "            print(f'{Fore.GREEN}EPOCH {epoch_number + 1}:{Style.RESET_ALL}')\n",
    "            \n",
    "            # Train for one epoch\n",
    "            self.train_one_epoch(train_loader)\n",
    "\n",
    "            # Validate on the validation subset\n",
    "            print(f'{Fore.CYAN}Validation :{Style.RESET_ALL}')\n",
    "            self.evaluate(val_loader)\n",
    "            print(f'{Fore.YELLOW}Test :{Style.RESET_ALL}')\n",
    "            self.evaluate(test_loader)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "generator = DataGenerator(raw_data.iloc[:64*4], filepath)\n",
    "model = Model(27904, 16, 2, device)\n",
    "\n",
    "dataloader = create_dataloader(generator)\n",
    "model.evaluate(dataloader)\n",
    "\n",
    "# print(\"Nombre de paramètres du model:\", model.parameters_number())\n",
    "\n",
    "# model.train_loop(generator, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████| 4/4 [01:49<00:00, 27.47s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m82.81%\u001b[0m, Recall: \u001b[32m100.00%\u001b[0m, F1 Score: \u001b[32m90.60%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m0.00%\u001b[0m, Recall: \u001b[32m0.00%\u001b[0m, F1 Score: \u001b[32m0.00%\u001b[0m\n",
      "Score : 0.16307693719863892\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model = torch.load(\"checkpoint1.pth\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style\n",
    "from data_processing import *\n",
    "from transformers import Wav2Vec2Model\n",
    "from transformers import CamembertModel\n",
    "from icecream import ic\n",
    "\n",
    "########################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "wave2vec_name = \"facebook/wav2vec2-base-960h\"\n",
    "wave2vec_model = Wav2Vec2Model.from_pretrained(wave2vec_name)\n",
    "wave2vec_model = wave2vec_model.to(device)\n",
    "\n",
    "bert_name = 'camembert-base'\n",
    "bert_model = CamembertModel.from_pretrained(bert_name)\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "for param in wave2vec_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "########################################################################\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model using covolutional neural net architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, device):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define model components\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "###################\n",
    "        \n",
    "    def parameters_number(self):\n",
    "        return(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Define the forward pass using the model components\n",
    "        \n",
    "        input_data['audio'] = input_data['audio'].to(self.device)\n",
    "        input_data['text'] = input_data['text'].to(self.device)\n",
    "        input_data['label'] = input_data['label'].to(self.device)\n",
    "\n",
    "        channel0 = input_data['audio'][:,0,:]\n",
    "        channel1 = input_data['audio'][:,1,:]\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_model(channel0)\n",
    "        wave2vec_output1 = wave2vec_model(channel1)\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_output0.last_hidden_state\n",
    "        wave2vec_output1 = wave2vec_output1.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.nn.functional.max_pool1d(wave2vec_output0, kernel_size=6)\n",
    "        wave2vec_output1 = torch.nn.functional.max_pool1d(wave2vec_output1, kernel_size=6)\n",
    "        \n",
    "        bert_output = bert_model(input_data['text'])\n",
    "        bert_output = bert_output.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.flatten(wave2vec_output0, start_dim=1)\n",
    "        wave2vec_output1 = torch.flatten(wave2vec_output1, start_dim=1)\n",
    "        bert_output = torch.flatten(bert_output, start_dim=1)\n",
    "\n",
    "        # Concatenate or combine the outputs as needed\n",
    "        combined_output = torch.cat((wave2vec_output0, wave2vec_output1, bert_output), dim=1)\n",
    "\n",
    "        # Apply linear layers\n",
    "        linear1_output = torch.relu(self.linear1(combined_output))\n",
    "        final_output = self.linear2(self.dropout(linear1_output))\n",
    "\n",
    "        return torch.softmax(final_output, dim=1)\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.eval()\n",
    "\n",
    "        # Define the loss function\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        true_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_negative_1 = torch.tensor(0).to(self.device)\n",
    "        \n",
    "        true_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_negative_0 = torch.tensor(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Validating\")):\n",
    "                \n",
    "                output = self.forward(batch)\n",
    "                labels = batch[\"label\"].long()\n",
    "\n",
    "                loss = loss_function(output, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                \n",
    "                true_positive_1 += torch.sum((predicted == labels) * (labels == 1))\n",
    "                false_positive_1 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 1))\n",
    "                false_negative_1 += torch.sum(((1-predicted) == labels) * (labels == 1))\n",
    "                \n",
    "                true_positive_0 += torch.sum((predicted == labels) * (labels == 0))\n",
    "                false_positive_0 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 0))\n",
    "                false_negative_0 += torch.sum(((1-predicted) == labels) * (labels == 0))\n",
    "                \n",
    "        precision_1 = true_positive_1 / max((true_positive_1 + false_positive_1), 1)\n",
    "        recall_1 = true_positive_1 / max((true_positive_1 + false_negative_1), 1) \n",
    "        \n",
    "        precision_0 = true_positive_0 / max((true_positive_0 + false_positive_0), 1)\n",
    "        recall_0 = true_positive_0 / max((true_positive_0 + false_negative_0), 1) \n",
    "        \n",
    "        f1_1 = 2 * (precision_1 * recall_1) / max((precision_1 + recall_1), 1)\n",
    "        f1_0 = 2 * (precision_0 * recall_0) / max((precision_0 + recall_0), 1)\n",
    "        \n",
    "        print(f\"Classe {Fore.RED}0{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_0 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_0 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_0 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Classe {Fore.RED}1{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_1 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_1 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_1 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Score : {(f1_0*0.18 + f1_1*(1-0.18))}\")\n",
    "                        \n",
    "    def train_one_epoch(self, dataloader):\n",
    "        \n",
    "        self.train(True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        # loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        try:\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "                \n",
    "                try:\n",
    "                \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    output = self.forward(batch)\n",
    "                    labels = batch[\"label\"].long()\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    \n",
    "                    true_positive_0 = (output * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "                    false_positive_0 = ((1 - output) * labels.unsqueeze(1))[:, 0].sum()\n",
    "                    false_negative_0 = ((1 - output) * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "\n",
    "                    true_positive_1 = (output * labels.unsqueeze(1))[:, 1].sum()\n",
    "                    false_positive_1 = (output * (1 - labels).unsqueeze(1))[:, 1].sum()\n",
    "                    false_negative_1 = ((1 - output) * labels.unsqueeze(1))[:, 1].sum()\n",
    "\n",
    "                    epsilon = torch.tensor([1e-7]).to(self.device) # 1e-7\n",
    "                    \n",
    "                    precision_0 = true_positive_0 / (true_positive_0 + false_positive_0 + epsilon)\n",
    "                    recall_0 = true_positive_0 / (true_positive_0 + false_negative_0 + epsilon)\n",
    "                    f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0 + epsilon)\n",
    "\n",
    "                    precision_1 = true_positive_1 / (true_positive_1 + false_positive_1 + epsilon)\n",
    "                    recall_1 = true_positive_1 / (true_positive_1 + false_negative_1 + epsilon)\n",
    "                    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1 + epsilon)\n",
    "                \n",
    "                    loss = -torch.log((f1_0*0.18 + f1_1*(1-0.18)) + epsilon)\n",
    "                    \n",
    "                    # ic(output, labels)\n",
    "                    # ic(true_positive_0, false_positive_0, false_negative_0, true_positive_1, false_positive_1, false_negative_1)\n",
    "                    # ic(precision_0, recall_0, f1_0, precision_1, recall_1, f1_1)\n",
    "                    # print(\"Successfully trained a batch\")\n",
    "                    \n",
    "                    ic(loss)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # labels = batch[\"label\"].long()\n",
    "                    # loss = loss_function(output, labels)\n",
    "                    # loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                except StopIteration as e:\n",
    "                    print(f\"An error occurred: {e}\")   \n",
    "                    # Catch StopIteration and continue to the next batch\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            # Handle other exceptions if needed\n",
    "            print(f\"An error occurred: {e}\")      \n",
    "\n",
    "    def train_loop(self, generator, nb_epoch):\n",
    "        \n",
    "        self.to(self.device)\n",
    "\n",
    "        data = generator.raw_data\n",
    "\n",
    "        train_data = data.sample(frac=0.9,random_state=200)\n",
    "        test_data = data.drop(train_data.index)\n",
    "\n",
    "        test_data.reset_index(drop=True, inplace=True)\n",
    "        train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_generator = DataGenerator(test_data, filepath)\n",
    "        test_loader = create_dataloader(test_generator)\n",
    "        \n",
    "        for epoch_number in range(nb_epoch):\n",
    "\n",
    "            train_subdata = train_data.sample(frac=0.9,random_state=200)\n",
    "            val_subdata = train_data.drop(train_subdata.index)\n",
    "            \n",
    "            train_subdata.reset_index(drop=True, inplace=True)\n",
    "            val_subdata.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            train_subgenerator = DataGenerator(train_subdata, filepath)\n",
    "            val_subgenerator = DataGenerator(val_subdata, filepath)\n",
    "\n",
    "            train_loader = create_dataloader(train_subgenerator)\n",
    "            val_loader = create_dataloader(val_subgenerator)\n",
    "\n",
    "            print(\"\")    \n",
    "            print(f'{Fore.GREEN}EPOCH {epoch_number + 1}:{Style.RESET_ALL}')\n",
    "            \n",
    "            # Train for one epoch\n",
    "            self.train_one_epoch(train_loader)\n",
    "\n",
    "            # Validate on the validation subset\n",
    "            print(f'{Fore.CYAN}Validation :{Style.RESET_ALL}')\n",
    "            self.evaluate(val_loader)\n",
    "            print(f'{Fore.YELLOW}Test :{Style.RESET_ALL}')\n",
    "            self.evaluate(test_loader)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "generator = DataGenerator(raw_data.iloc[4096:8192], filepath)\n",
    "\n",
    "# model = Model(27904, 16, 2, device)\n",
    "# dataloader = create_dataloader(generator)\n",
    "# model.evaluate(dataloader)\n",
    "\n",
    "# print(\"Nombre de paramètres du model:\", model.parameters_number())\n",
    "\n",
    "# model.train_loop(generator, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "model.train_loop(generator, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u001b[32mEPOCH 1:\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/51 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "An error occurred: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.\n",
      "\u001b[36mValidation :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "model = torch.load(\"checkpoint1\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mInteractiveInput-1\u001b[0m in \u001b[0;36mline 1\n\u001b[1;32m----> <a href='vscode-interactive-input:/InteractiveInput-1?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mcheckpoint1\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style\n",
    "from data_processing import *\n",
    "from transformers import Wav2Vec2Model\n",
    "from transformers import CamembertModel\n",
    "from icecream import ic\n",
    "\n",
    "########################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "wave2vec_name = \"facebook/wav2vec2-base-960h\"\n",
    "wave2vec_model = Wav2Vec2Model.from_pretrained(wave2vec_name)\n",
    "wave2vec_model = wave2vec_model.to(device)\n",
    "\n",
    "bert_name = 'camembert-base'\n",
    "bert_model = CamembertModel.from_pretrained(bert_name)\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "for param in wave2vec_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "########################################################################\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model using covolutional neural net architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, device):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Define model components\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "###################\n",
    "        \n",
    "    def parameters_number(self):\n",
    "        return(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        # Define the forward pass using the model components\n",
    "        \n",
    "        input_data['audio'] = input_data['audio'].to(self.device)\n",
    "        input_data['text'] = input_data['text'].to(self.device)\n",
    "        input_data['label'] = input_data['label'].to(self.device)\n",
    "\n",
    "        channel0 = input_data['audio'][:,0,:]\n",
    "        channel1 = input_data['audio'][:,1,:]\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_model(channel0)\n",
    "        wave2vec_output1 = wave2vec_model(channel1)\n",
    "        \n",
    "        wave2vec_output0 = wave2vec_output0.last_hidden_state\n",
    "        wave2vec_output1 = wave2vec_output1.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.nn.functional.max_pool1d(wave2vec_output0, kernel_size=6)\n",
    "        wave2vec_output1 = torch.nn.functional.max_pool1d(wave2vec_output1, kernel_size=6)\n",
    "        \n",
    "        bert_output = bert_model(input_data['text'])\n",
    "        bert_output = bert_output.last_hidden_state\n",
    "        \n",
    "        wave2vec_output0 = torch.flatten(wave2vec_output0, start_dim=1)\n",
    "        wave2vec_output1 = torch.flatten(wave2vec_output1, start_dim=1)\n",
    "        bert_output = torch.flatten(bert_output, start_dim=1)\n",
    "\n",
    "        # Concatenate or combine the outputs as needed\n",
    "        combined_output = torch.cat((wave2vec_output0, wave2vec_output1, bert_output), dim=1)\n",
    "\n",
    "        # Apply linear layers\n",
    "        linear1_output = torch.relu(self.linear1(combined_output))\n",
    "        final_output = self.linear2(self.dropout(linear1_output))\n",
    "\n",
    "        return torch.softmax(final_output, dim=1)\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.eval()\n",
    "\n",
    "        # Define the loss function\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        true_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_positive_1 = torch.tensor(0).to(self.device)\n",
    "        false_negative_1 = torch.tensor(0).to(self.device)\n",
    "        \n",
    "        true_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_positive_0 = torch.tensor(0).to(self.device)\n",
    "        false_negative_0 = torch.tensor(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Validating\")):\n",
    "                \n",
    "                output = self.forward(batch)\n",
    "                labels = batch[\"label\"].long()\n",
    "\n",
    "                loss = loss_function(output, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                \n",
    "                true_positive_1 += torch.sum((predicted == labels) * (labels == 1))\n",
    "                false_positive_1 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 1))\n",
    "                false_negative_1 += torch.sum(((1-predicted) == labels) * (labels == 1))\n",
    "                \n",
    "                true_positive_0 += torch.sum((predicted == labels) * (labels == 0))\n",
    "                false_positive_0 += torch.sum((predicted == (1 - labels)) * ((1 - labels) == 0))\n",
    "                false_negative_0 += torch.sum(((1-predicted) == labels) * (labels == 0))\n",
    "                \n",
    "        precision_1 = true_positive_1 / max((true_positive_1 + false_positive_1), 1)\n",
    "        recall_1 = true_positive_1 / max((true_positive_1 + false_negative_1), 1) \n",
    "        \n",
    "        precision_0 = true_positive_0 / max((true_positive_0 + false_positive_0), 1)\n",
    "        recall_0 = true_positive_0 / max((true_positive_0 + false_negative_0), 1) \n",
    "        \n",
    "        f1_1 = 2 * (precision_1 * recall_1) / max((precision_1 + recall_1), 1)\n",
    "        f1_0 = 2 * (precision_0 * recall_0) / max((precision_0 + recall_0), 1)\n",
    "        \n",
    "        print(f\"Classe {Fore.RED}0{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_0 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_0 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_0 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Classe {Fore.RED}1{Style.RESET_ALL} | Precision: {Fore.GREEN}{precision_1 * 100:.2f}%{Style.RESET_ALL}, Recall: {Fore.GREEN}{recall_1 * 100:.2f}%{Style.RESET_ALL}, F1 Score: {Fore.GREEN}{f1_1 * 100:.2f}%{Style.RESET_ALL}\")\n",
    "        print(f\"Score : {(f1_0*0.18 + f1_1*(1-0.18))}\")\n",
    "                        \n",
    "    def train_one_epoch(self, dataloader):\n",
    "        \n",
    "        self.train(True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        # loss_function = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        try:\n",
    "            for _, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "                \n",
    "                try:\n",
    "                \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    output = self.forward(batch)\n",
    "                    labels = batch[\"label\"].long()\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    \n",
    "                    true_positive_0 = (output * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "                    false_positive_0 = ((1 - output) * labels.unsqueeze(1))[:, 0].sum()\n",
    "                    false_negative_0 = ((1 - output) * (1 - labels).unsqueeze(1))[:, 0].sum()\n",
    "\n",
    "                    true_positive_1 = (output * labels.unsqueeze(1))[:, 1].sum()\n",
    "                    false_positive_1 = (output * (1 - labels).unsqueeze(1))[:, 1].sum()\n",
    "                    false_negative_1 = ((1 - output) * labels.unsqueeze(1))[:, 1].sum()\n",
    "\n",
    "                    epsilon = torch.tensor([1e-7]).to(self.device) # 1e-7\n",
    "                    \n",
    "                    precision_0 = true_positive_0 / (true_positive_0 + false_positive_0 + epsilon)\n",
    "                    recall_0 = true_positive_0 / (true_positive_0 + false_negative_0 + epsilon)\n",
    "                    f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0 + epsilon)\n",
    "\n",
    "                    precision_1 = true_positive_1 / (true_positive_1 + false_positive_1 + epsilon)\n",
    "                    recall_1 = true_positive_1 / (true_positive_1 + false_negative_1 + epsilon)\n",
    "                    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1 + epsilon)\n",
    "                \n",
    "                    loss = -torch.log((f1_0*0.18 + f1_1*(1-0.18)) + epsilon)\n",
    "                    \n",
    "                    # ic(output, labels)\n",
    "                    # ic(true_positive_0, false_positive_0, false_negative_0, true_positive_1, false_positive_1, false_negative_1)\n",
    "                    # ic(precision_0, recall_0, f1_0, precision_1, recall_1, f1_1)\n",
    "                    # print(\"Successfully trained a batch\")\n",
    "                    \n",
    "                    ic(loss)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # labels = batch[\"label\"].long()\n",
    "                    # loss = loss_function(output, labels)\n",
    "                    # loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                except StopIteration as e:\n",
    "                    print(f\"An error occurred: {e}\")   \n",
    "                    # Catch StopIteration and continue to the next batch\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            # Handle other exceptions if needed\n",
    "            print(f\"An error occurred: {e}\")      \n",
    "\n",
    "    def train_loop(self, generator, nb_epoch):\n",
    "        \n",
    "        self.to(self.device)\n",
    "\n",
    "        data = generator.raw_data\n",
    "\n",
    "        train_data = data.sample(frac=0.9,random_state=200)\n",
    "        test_data = data.drop(train_data.index)\n",
    "\n",
    "        test_data.reset_index(drop=True, inplace=True)\n",
    "        train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_generator = DataGenerator(test_data, filepath)\n",
    "        test_loader = create_dataloader(test_generator)\n",
    "        \n",
    "        for epoch_number in range(nb_epoch):\n",
    "\n",
    "            train_subdata = train_data.sample(frac=0.9,random_state=200)\n",
    "            val_subdata = train_data.drop(train_subdata.index)\n",
    "            \n",
    "            train_subdata.reset_index(drop=True, inplace=True)\n",
    "            val_subdata.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            train_subgenerator = DataGenerator(train_subdata, filepath)\n",
    "            val_subgenerator = DataGenerator(val_subdata, filepath)\n",
    "\n",
    "            train_loader = create_dataloader(train_subgenerator)\n",
    "            val_loader = create_dataloader(val_subgenerator)\n",
    "\n",
    "            print(\"\")    \n",
    "            print(f'{Fore.GREEN}EPOCH {epoch_number + 1}:{Style.RESET_ALL}')\n",
    "            \n",
    "            # Train for one epoch\n",
    "            self.train_one_epoch(train_loader)\n",
    "\n",
    "            # Validate on the validation subset\n",
    "            print(f'{Fore.CYAN}Validation :{Style.RESET_ALL}')\n",
    "            self.evaluate(val_loader)\n",
    "            print(f'{Fore.YELLOW}Test :{Style.RESET_ALL}')\n",
    "            self.evaluate(test_loader)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "generator = DataGenerator(raw_data.iloc[8192:8192+4096], filepath)\n",
    "\n",
    "# model = Model(27904, 16, 2, device)\n",
    "# dataloader = create_dataloader(generator)\n",
    "# model.evaluate(dataloader)\n",
    "\n",
    "# print(\"Nombre de paramètres du model:\", model.parameters_number())\n",
    "\n",
    "# model.train_loop(generator, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16400, 12)\n",
      "is_main_speaker      0.739207\n",
      "turn_at_start        0.184756\n",
      "turn_after           0.184756\n",
      "turn_start_word     10.250372\n",
      "yield_at_end         0.188415\n",
      "request_at_start     0.198537\n",
      "dtype: float64\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model = torch.load(\"checkpoint1\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoint1'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mInteractiveInput-1\u001b[0m in \u001b[0;36mline 1\n\u001b[1;32m----> <a href='vscode-interactive-input:/InteractiveInput-1?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mcheckpoint1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=982'>983</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=983'>984</a>\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=985'>986</a>\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=986'>987</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=987'>988</a>\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=988'>989</a>\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=989'>990</a>\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=990'>991</a>\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=432'>433</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=433'>434</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=434'>435</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=435'>436</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=436'>437</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=414'>415</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/serialization.py?line=415'>416</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoint1'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "model = torch.load(\"checkpoint1.pth\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model.train_loop(generator, 5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u001b[32mEPOCH 1:\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/51 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "ic| loss: tensor([1.0002], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   2%|▏         | 1/51 [00:47<39:53, 47.88s/it]ic| loss: tensor([0.6875], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   4%|▍         | 2/51 [01:31<37:14, 45.60s/it]ic| loss: tensor([1.3246], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   6%|▌         | 3/51 [02:13<34:58, 43.71s/it]ic| loss: tensor([1.1298], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   8%|▊         | 4/51 [02:52<32:56, 42.06s/it]ic| loss: tensor([0.7041], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  10%|▉         | 5/51 [03:31<31:16, 40.79s/it]ic| loss: tensor([1.1198], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  12%|█▏        | 6/51 [04:13<30:57, 41.27s/it]ic| loss: tensor([0.7973], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  14%|█▎        | 7/51 [04:54<30:13, 41.22s/it]ic| loss: tensor([0.7003], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  16%|█▌        | 8/51 [05:35<29:24, 41.03s/it]ic| loss: tensor([1.1081], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  18%|█▊        | 9/51 [06:14<28:17, 40.43s/it]ic| loss: tensor([0.8553], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  20%|█▉        | 10/51 [06:55<27:49, 40.72s/it]ic| loss: tensor([0.8261], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  22%|██▏       | 11/51 [07:35<26:52, 40.32s/it]ic| loss: tensor([1.0767], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  24%|██▎       | 12/51 [08:13<25:44, 39.61s/it]ic| loss: tensor([0.7751], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  25%|██▌       | 13/51 [08:53<25:17, 39.93s/it]ic| loss: tensor([0.6481], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  27%|██▋       | 14/51 [09:33<24:37, 39.92s/it]ic| loss: tensor([1.0119], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  29%|██▉       | 15/51 [10:16<24:24, 40.67s/it]ic| loss: tensor([0.8517], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  31%|███▏      | 16/51 [10:59<24:09, 41.40s/it]ic| loss: tensor([1.0376], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  33%|███▎      | 17/51 [11:44<24:05, 42.51s/it]ic| loss: tensor([0.8216], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  35%|███▌      | 18/51 [12:42<25:53, 47.09s/it]ic| loss: tensor([0.7281], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  37%|███▋      | 19/51 [13:24<24:18, 45.58s/it]ic| loss: tensor([0.7885], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  39%|███▉      | 20/51 [14:04<22:46, 44.08s/it]ic| loss: tensor([0.6553], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  41%|████      | 21/51 [14:49<22:03, 44.12s/it]ic| loss: tensor([0.9054], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  43%|████▎     | 22/51 [15:29<20:49, 43.08s/it]ic| loss: tensor([0.9719], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  45%|████▌     | 23/51 [16:10<19:50, 42.53s/it]ic| loss: tensor([0.7781], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  47%|████▋     | 24/51 [16:53<19:11, 42.66s/it]ic| loss: tensor([0.7464], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  49%|████▉     | 25/51 [17:35<18:20, 42.32s/it]ic| loss: tensor([0.8996], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  51%|█████     | 26/51 [18:15<17:22, 41.71s/it]ic| loss: tensor([0.7434], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  53%|█████▎    | 27/51 [18:57<16:39, 41.66s/it]ic| loss: tensor([0.8844], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  55%|█████▍    | 28/51 [20:11<16:35, 43.27s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "An error occurred: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "\u001b[36mValidation :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 5/5 [03:30<00:00, 42.04s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m81.47%\u001b[0m, Recall: \u001b[32m84.40%\u001b[0m, F1 Score: \u001b[32m82.91%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m36.07%\u001b[0m, Recall: \u001b[32m31.43%\u001b[0m, F1 Score: \u001b[32m22.67%\u001b[0m\n",
      "Score : 0.33512604236602783\n",
      "\u001b[33mTest :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 6/6 [03:58<00:00, 39.79s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m87.91%\u001b[0m, Recall: \u001b[32m83.02%\u001b[0m, F1 Score: \u001b[32m85.40%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m29.49%\u001b[0m, Recall: \u001b[32m38.33%\u001b[0m, F1 Score: \u001b[32m22.61%\u001b[0m\n",
      "Score : 0.33909034729003906\n",
      "\n",
      "\u001b[32mEPOCH 2:\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/51 [00:00<?, ?it/s]ic| loss: tensor([0.6685], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   2%|▏         | 1/51 [00:40<33:34, 40.29s/it]ic| loss: tensor([0.8052], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   4%|▍         | 2/51 [01:18<31:57, 39.12s/it]ic| loss: tensor([0.9618], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   6%|▌         | 3/51 [01:59<31:55, 39.91s/it]ic| loss: tensor([0.7577], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   8%|▊         | 4/51 [02:43<32:40, 41.70s/it]ic| loss: tensor([0.8855], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  10%|▉         | 5/51 [03:26<32:18, 42.13s/it]ic| loss: tensor([0.6924], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  12%|█▏        | 6/51 [04:09<31:40, 42.23s/it]ic| loss: tensor([0.8990], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  14%|█▎        | 7/51 [04:52<31:14, 42.61s/it]ic| loss: tensor([1.0291], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  16%|█▌        | 8/51 [05:37<30:57, 43.21s/it]ic| loss: tensor([1.0839], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  18%|█▊        | 9/51 [06:45<31:30, 45.02s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "An error occurred: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "\u001b[36mValidation :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 5/5 [02:49<00:00, 33.84s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m82.95%\u001b[0m, Recall: \u001b[32m72.87%\u001b[0m, F1 Score: \u001b[32m77.59%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m34.95%\u001b[0m, Recall: \u001b[32m49.32%\u001b[0m, F1 Score: \u001b[32m34.47%\u001b[0m\n",
      "Score : 0.4223310947418213\n",
      "\u001b[33mTest :\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: 100%|██████████| 6/6 [03:52<00:00, 38.75s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classe \u001b[31m0\u001b[0m | Precision: \u001b[32m89.52%\u001b[0m, Recall: \u001b[32m68.73%\u001b[0m, F1 Score: \u001b[32m77.76%\u001b[0m\n",
      "Classe \u001b[31m1\u001b[0m | Precision: \u001b[32m25.74%\u001b[0m, Recall: \u001b[32m57.38%\u001b[0m, F1 Score: \u001b[32m29.53%\u001b[0m\n",
      "Score : 0.38212987780570984\n",
      "\n",
      "\u001b[32mEPOCH 3:\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/51 [00:00<?, ?it/s]ic| loss: tensor([0.7296], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   2%|▏         | 1/51 [00:41<34:50, 41.81s/it]ic| loss: tensor([0.7643], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   4%|▍         | 2/51 [01:22<33:36, 41.15s/it]ic| loss: tensor([0.9059], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   6%|▌         | 3/51 [02:02<32:33, 40.70s/it]ic| loss: tensor([0.7504], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:   8%|▊         | 4/51 [02:49<33:42, 43.02s/it]ic| loss: tensor([0.6704], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  10%|▉         | 5/51 [03:25<31:02, 40.49s/it]ic| loss: tensor([0.8376], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  12%|█▏        | 6/51 [03:54<27:25, 36.56s/it]ic| loss: tensor([0.9075], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  14%|█▎        | 7/51 [04:36<28:05, 38.30s/it]ic| loss: tensor([1.1585], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  16%|█▌        | 8/51 [05:22<29:22, 40.99s/it]ic| loss: tensor([1.0926], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  18%|█▊        | 9/51 [06:08<29:39, 42.36s/it]ic| loss: tensor([0.8457], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  20%|█▉        | 10/51 [06:53<29:36, 43.34s/it]ic| loss: tensor([1.1678], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  22%|██▏       | 11/51 [07:37<28:55, 43.38s/it]ic| loss: tensor([0.7689], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  24%|██▎       | 12/51 [08:22<28:36, 44.02s/it]ic| loss: tensor([0.9271], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  25%|██▌       | 13/51 [09:07<27:59, 44.21s/it]ic| loss: tensor([0.7128], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  27%|██▋       | 14/51 [09:50<27:09, 44.03s/it]ic| loss: tensor([0.8724], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  29%|██▉       | 15/51 [10:37<26:48, 44.68s/it]ic| loss: tensor([0.7813], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  31%|███▏      | 16/51 [11:21<25:59, 44.56s/it]ic| loss: tensor([0.7515], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  33%|███▎      | 17/51 [12:06<25:22, 44.78s/it]ic| loss: tensor([0.7569], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  35%|███▌      | 18/51 [12:51<24:37, 44.77s/it]ic| loss: tensor([0.6450], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  37%|███▋      | 19/51 [13:42<24:52, 46.64s/it]ic| loss: tensor([0.6011], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  39%|███▉      | 20/51 [14:27<23:50, 46.16s/it]ic| loss: tensor([0.9664], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  41%|████      | 21/51 [15:12<22:57, 45.91s/it]ic| loss: tensor([0.5938], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Training:  43%|████▎     | 22/51 [16:02<21:08, 43.75s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mtrain_loop(generator, \u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\model.py\u001b[0m in \u001b[0;36mline 230\u001b[0m, in \u001b[0;36mModel.train_loop\u001b[1;34m(self, generator, nb_epoch)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=226'>227</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mFore\u001b[39m.\u001b[39mGREEN\u001b[39m}\u001b[39;00m\u001b[39mEPOCH \u001b[39m\u001b[39m{\u001b[39;00mepoch_number\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m{\u001b[39;00mStyle\u001b[39m.\u001b[39mRESET_ALL\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=228'>229</a>\u001b[0m \u001b[39m# Train for one epoch\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=229'>230</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_one_epoch(train_loader)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=231'>232</a>\u001b[0m \u001b[39m# Validate on the validation subset\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=232'>233</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mFore\u001b[39m.\u001b[39mCYAN\u001b[39m}\u001b[39;00m\u001b[39mValidation :\u001b[39m\u001b[39m{\u001b[39;00mStyle\u001b[39m.\u001b[39mRESET_ALL\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\model.py\u001b[0m in \u001b[0;36mline 145\u001b[0m, in \u001b[0;36mModel.train_one_epoch\u001b[1;34m(self, dataloader)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=141'>142</a>\u001b[0m \u001b[39m# loss_function = torch.nn.CrossEntropyLoss()\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=143'>144</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=144'>145</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(dataloader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=146'>147</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/model.py?line=148'>149</a>\u001b[0m             optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1174'>1175</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1176'>1177</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1177'>1178</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1178'>1179</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1179'>1180</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/tqdm/std.py?line=1180'>1181</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=626'>627</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=627'>628</a>\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=628'>629</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=629'>630</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=630'>631</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=631'>632</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=632'>633</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=633'>634</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=671'>672</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=672'>673</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=673'>674</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=674'>675</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/dataloader.py?line=675'>676</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=52'>53</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/torch/utils/data/_utils/fetch.py?line=52'>53</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\data_processing.py:47\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=45'>46</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, i):\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m : get_audio(i, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath),\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=47'>48</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m : get_text(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_data),\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=48'>49</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m : get_label(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_data)}\n",
      "File \u001b[1;32mc:\\Users\\cranc\\Documents\\Documents\\Ecole\\3A\\2eme Temps\\SAM\\SAM_Project\\data_processing.py:20\u001b[0m, in \u001b[0;36mget_audio\u001b[1;34m(i, raw_data, filepath)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=17'>18</a>\u001b[0m audio_file_path \u001b[39m=\u001b[39m  filepath \u001b[39m+\u001b[39m raw_data[\u001b[39m\"\u001b[39m\u001b[39mdyad\u001b[39m\u001b[39m\"\u001b[39m][i]\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mtranscr\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.wav\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=18'>19</a>\u001b[0m audio_tensor, sampling_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mload(audio_file_path)\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=19'>20</a>\u001b[0m audio_tensor \u001b[39m=\u001b[39m processor(audio_tensor, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, sampling_rate \u001b[39m=\u001b[39;49m sampling_rate)\u001b[39m.\u001b[39minput_values\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=20'>21</a>\u001b[0m stop \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(sampling_rate\u001b[39m*\u001b[39mraw_data[\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m][i])\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/Documents/Documents/Ecole/3A/2eme%20Temps/SAM/SAM_Project/data_processing.py?line=21'>22</a>\u001b[0m start \u001b[39m=\u001b[39m stop\u001b[39m-\u001b[39msampling_rate\u001b[39m*\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:94\u001b[0m, in \u001b[0;36mWav2Vec2Processor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=90'>91</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou need to specify either an `audio` or `text` input to process.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=92'>93</a>\u001b[0m \u001b[39mif\u001b[39;00m audio \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=93'>94</a>\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_extractor(audio, \u001b[39m*\u001b[39;49margs, sampling_rate\u001b[39m=\u001b[39;49msampling_rate, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=94'>95</a>\u001b[0m \u001b[39mif\u001b[39;00m text \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py?line=95'>96</a>\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(text, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py:233\u001b[0m, in \u001b[0;36mWav2Vec2FeatureExtractor.__call__\u001b[1;34m(self, raw_speech, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors, sampling_rate, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=226'>227</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_normalize:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=227'>228</a>\u001b[0m     attention_mask \u001b[39m=\u001b[39m (\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=228'>229</a>\u001b[0m         attention_mask\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=229'>230</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_strategies(padding, max_length\u001b[39m=\u001b[39mmax_length) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=230'>231</a>\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=231'>232</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=232'>233</a>\u001b[0m     padded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_values\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzero_mean_unit_var_norm(\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=233'>234</a>\u001b[0m         padded_inputs[\u001b[39m\"\u001b[39;49m\u001b[39minput_values\u001b[39;49m\u001b[39m\"\u001b[39;49m], attention_mask\u001b[39m=\u001b[39;49mattention_mask, padding_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_value\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=234'>235</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=236'>237</a>\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=237'>238</a>\u001b[0m     padded_inputs \u001b[39m=\u001b[39m padded_inputs\u001b[39m.\u001b[39mconvert_to_tensors(return_tensors)\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py:98\u001b[0m, in \u001b[0;36mWav2Vec2FeatureExtractor.zero_mean_unit_var_norm\u001b[1;34m(input_values, attention_mask, padding_value)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=95'>96</a>\u001b[0m         normed_input_values\u001b[39m.\u001b[39mappend(normed_slice)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=96'>97</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=97'>98</a>\u001b[0m     normed_input_values \u001b[39m=\u001b[39m [(x \u001b[39m-\u001b[39;49m x\u001b[39m.\u001b[39;49mmean()) \u001b[39m/\u001b[39;49m np\u001b[39m.\u001b[39;49msqrt(x\u001b[39m.\u001b[39;49mvar() \u001b[39m+\u001b[39;49m \u001b[39m1e-7\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m input_values]\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=99'>100</a>\u001b[0m \u001b[39mreturn\u001b[39;00m normed_input_values\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py:98\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=95'>96</a>\u001b[0m         normed_input_values\u001b[39m.\u001b[39mappend(normed_slice)\n\u001b[0;32m     <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=96'>97</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=97'>98</a>\u001b[0m     normed_input_values \u001b[39m=\u001b[39m [(x \u001b[39m-\u001b[39m x\u001b[39m.\u001b[39mmean()) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msqrt(x\u001b[39m.\u001b[39;49mvar() \u001b[39m+\u001b[39m \u001b[39m1e-7\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m input_values]\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py?line=99'>100</a>\u001b[0m \u001b[39mreturn\u001b[39;00m normed_input_values\n",
      "File \u001b[1;32mc:\\Users\\cranc\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:196\u001b[0m, in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=191'>192</a>\u001b[0m         ret \u001b[39m=\u001b[39m ret \u001b[39m/\u001b[39m rcount\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=193'>194</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n\u001b[1;32m--> <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=195'>196</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_var\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ddof\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=196'>197</a>\u001b[0m          where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=197'>198</a>\u001b[0m     arr \u001b[39m=\u001b[39m asanyarray(a)\n\u001b[0;32m    <a href='file:///c%3A/Users/cranc/anaconda3/Lib/site-packages/numpy/core/_methods.py?line=199'>200</a>\u001b[0m     rcount \u001b[39m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[39m=\u001b[39mkeepdims, where\u001b[39m=\u001b[39mwhere)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "torch.save(model, \"checkpoint2.pth\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}